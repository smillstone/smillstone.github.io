<!DOCTYPE html>
<html>
<style>
body {
    background-image: url('https://media.istockphoto.com/photos/balsa-wood-picture-id493483420?k=6&m=493483420&s=612x612&w=0&h=r8bz4D_nq44viWkYtQaHx6mZjG6vOOuQtIdb6S5lMA8=');
}
</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<body>
<h1>A508 Class Summary</h1>
<h2>Sam Millstone</h2>
<h3>Purpose of this web page:</h3>
<p>This page summarizes relevant concepts and statistical techniques learned in A508 from the book Modern Statistical Methods for Astronomy. The focus is more on restating the ideas behind different techniques in a way that makes sense to me (specifically those that were mentioned more than just in passing), than just regurgitating information from the book.</p>
<h3>Table of Contents:</h3>
<ol>
    <li>Introduction</li>
    <li>Probability</li>
    <li>Statistical Inference</li>
    <li>Probability Distribution Functions</li>
    <li>Nonparametric Statistics</li>
    <li>Data Smoothing: Density Estimation</li>
    <li>Regression</li>
    <li>Multivariate Analysis</li>
    <li>Clustering, Classification, and Data Mining</li>
    <li>Nondetections: Censored and Truncated Data</li>
    <li>Time Series Analysis</li>
    <li>Spatial Point Processes</li>
</ol>
<h2>Introduction</h2>
<h3>1.1 The role of statistics in astronomy</h3>
<p>The meaning and goals of statistics are debated, but it's basically a tool for interpreting data. There is a lot of human influence when using statistics and how it applies to data, so everything will be up to the scientist's interpretation, to at least some degree.</p>
<p>Astronomers tend to believe that the underlying physical processes can be uncovered by analysis of observations. However, there are many challenges and options that one can use, and it is far too easy to reach different conclusions based on different interpretations and statistical analyses of the same data.</p>
<h3>1.2 History of statistics in astronomy</h3>
<p>Interesting, but not terribly relevant to the goal of this website.</p>
<h2>Probability</h2>
<h3>2.1 Uncertainty in observational science</h3>
<p>It is usually impossible to measure everything about every member of a population, and those measurements that are taken are not infinitely accurate or precise. There is always some limit to our knowledge, represented by uncertainty.</p>
<h3>2.2 Outcome spaces and events</h3>
<p>Most of the terms defined here are in common usage and are pretty obvious, but it can be helpful to define them in a specific way.</p>
<p><strong>Experiment</strong> - Any action whose results are not known with complete certainty before the action occurs.</p>
<p><strong>Outcome space or sample space</strong> - The collection of all possible outcomes of the experiment.</p>
<p><strong>Event</strong> - A subset of the sample space. <i>Not necessarily</i> a single outcome. In the sample space of all stars within 50 pc, one event could be all stars with an apparent magnitude &lt; 10, while another event tells whether a star is in a binary system.</p>
<p>The simplest probability is when all outcomes are equally likely, so the <strong>probability of an event occurring is simply the number of favorable outcomes divided by the total number of possible outcomes.</strong></p>
<p>For simple problems, sometimes you can just use logic to assign probabilities if the outcomes can be constructed out of outcomes from a simple experiment like the definition in bold.</p>
<p><strong>Discrete sample space</strong> - any finite or countably infinite sample space.</p>
<p>In general, the probability of an event occurring is equal to the sum of individual outcomes that are favorable to the event.</p>
<h3>2.3 Axioms of probability</h3>
<p><strong>Probability space</strong> - Described by three objects: Ω, F, and P. Ω is the sample space, F is the class of events (a collection of events from the sample space), and P is a function that assigns probabilities to each event in F.</p>
<p><strong>Axiom 1: The probability of any event must be between 0 and 1.</strong><br><strong>Axiom 2: The sum of all probabilities must equal 1.</strong><br><strong>Axiom 3: The probability of at least one of a set of mutually exclusive events occurring is the sum of their individual probabilities. This is known as </strong><i><strong>countable additivity</strong></i><strong>.</strong></p>
<p>The probability of the complement of an event is simply 1 minus the probability of the event.</p>
<p>The probability of the union of two events is equal to the sum of the individual event probabilities minus the probability of the intersection. This can be extended to the <strong>inclusion-exclusion formula</strong>.</p>
<h3>2.4 Conditional probabilities</h3>
<p>Necessary for understanding Bayesian statistics. The probabilities of events can sometimes depend on previous knowledge about the system. Defined in words, <strong>the probability of A given B equals the probability of the intersection of A and B divided by the probability of B.</strong> An extension of this definition is the</p>
<p><strong>Multiplication rule</strong> -&nbsp;</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://ibb.co/8BMy2Ls"><img src="https://i.ibb.co/cvJWtS3/image.png" alt="image" border="0"></a></p>
<p>This is a very important concept for astronomy, since there is almost always at least some previous knowledge available for any given object.</p>
<p>The multiplication rule also leads to the</p>
<p><strong>Law of total probability</strong> -&nbsp;</p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:530/46;" src="https://i.ibb.co/9bH92Bx/image.png" width="530" height="46"></figure>
<p>where Bi is a partition of the sample space. All of this finally leads to</p>
<p><strong>Theorem 2.1 (Bayes' Theorem):</strong></p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="E4oRCTp7PmhP">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/E4oRCTp7PmhP/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/E4oRCTp7PmhP/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/E4oRCTp7PmhP/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/E4oRCTp7PmhP/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/E4oRCTp7PmhP/images/347.webp 347w" type="image/webp" sizes="(max-width: 347px) 100vw, 347px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/E4oRCTp7PmhP/images/347.png" alt="" width="347" height="98">
    </picture>
</figure>
<p>where P(A) is defined by the law of total probability.</p>
<p>If P(A|B) = P(A), then A and B are <strong>independent</strong>. In other words, the probability of independent events occurring is the <i>product of the individual probabilities.</i></p>
<h3>2.5 Random variables</h3>
<p><strong>Random variables</strong> are functions on the sample space. For example, the number of heads that you get when flipping a coin four times is a random variable on the sample space of all sets of four coin flips.</p>
<p><strong>Cumulative distribution function</strong> - The probability as a function of x that the value of a random variable is less than or equal to x.</p>
<p><strong>Probability density function</strong> - The integral of the pdf over a range of values equals the probability of the random variable having a value within that range.</p>
<p>The idea of a random variable can be extended to a <strong>random vector</strong> of variables. Grouping variables this way allows you to study the relationships between variables instead of just each variable individually.</p>
<p><strong>Marginal distribution</strong> - The 1-D distributions of individual variables form the random vector. They act like a regular CDF.</p>
<p>Just like independent events, <strong>independent RVs</strong> have the joint distribution equal to the product of the marginal distributions.</p>
<p><strong>Moments</strong> - Mathematically, the kth moment of f(x) is the integral over the sample space of f(x) to the kth power times the pdf.<br><strong>Central Moments</strong> - Moments where the RV first has the expectation subtracted out</p>
<p><strong>Mean/First moment</strong> - also called the expected value or expectation value</p>
<p><strong>Variance </strong>- Second moment minus first moment squared, describes the spread of a function.<br><strong>Covariance</strong> - measures the relationships between scatter in two RVs. Independent RVs have Cov = 0.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="cJwoOixwTq00">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/680.webp 680w" type="image/webp" sizes="(max-width: 680px) 100vw, 680px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/cJwoOixwTq00/images/680.jpeg" width="680" height="58">
    </picture>
</figure>
<p><strong>Skewness</strong> - Third central moment, describes which direction the RV leans</p>
<p><strong>Homoscedastic</strong> - If all variables have the same variance. Opposite is <strong>heteroscedastic</strong>. For independent variables, the sample mean is the sum of all the RVs divided by the number of RVs, and the variance is the homoscedastic variance divided by the number of RVs.</p>
<p><strong>Standard deviation</strong> - the square root of the variance</p>
<p><strong>Standardized form</strong> of a variable - The variable minus the mean and divided by the standard deviation. This removes any units associated with the variable.</p>
<p><strong>Independent and identically distributed</strong> - i.i.d. for short. The assumption that data are all generated from the same population or underlying distribution. For example, if you repeat the same experiment multiple times. <strong>Many methods require i.i.d. variables, so be careful.</strong></p>
<h3>2.6 Quantile function</h3>
<p><strong>Quantile function</strong> - The inverse of the CDF. It is the measurement of the value of the RV at which a specific fraction of the CDF has passed. For example, the 95% quantile tells what the value of the random variable is when the CDF reaches 0.95.</p>
<h3>2.7 Discrete distributions</h3>
<p><strong>Bernoulli distribution</strong> - An experiment that can result in only two outcomes, where the probability of one is p and the other is 1-p.</p>
<p><strong>Binomial distribution</strong> - A Bernoulli trial is repeated n times independently. The binomial distribution describes the probability of a given number of successes. X ~ Bin(n,p). Mean = np. Variance = np(1-p)</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:507/78;" src="https://i.ibb.co/kMjXKhF/image.png" width="507" height="78"></figure>
<p><strong>Poisson distribution</strong> - A good approximation for binomial probabilities when p<sub>n</sub> is small, n is large, and λ=np<sub>n</sub> is a reasonable value. Describes many physical phenomena where counting statistics are important (photons hitting a detector, nuclear decay, etc.). Mean = Variance = λ</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:526/89;" src="https://i.ibb.co/SsbSv5z/image.png" width="526" height="89"></figure>
<p><strong>Negative binomial distribution</strong> - Known as the <strong>geometric distribution</strong> when r = 1. Mean = r/p, Variance = qr/p<sup>2</sup>.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="natJ8mw7U3V_">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/697.webp 697w" type="image/webp" sizes="(max-width: 697px) 100vw, 697px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/natJ8mw7U3V_/images/697.png" width="697" height="122">
    </picture>
</figure>
<p>for n=r, r+1, . . . ,</p>
<h3>2.8 Continuous distributions</h3>
<p><strong>Uniform distribution</strong> - constant between bounds a and b. <strong>f = 1/(b-a), a &lt; x &lt; b.</strong> Mean = (a+b)/2, Variance = (1/12)(b-a)<sup>2</sup>.</p>
<p><strong>Exponential distribution</strong> - <strong>F(x) = 1 - e<sup>-λx</sup>, f(x) = λe<sup>-λx</sup>, x ≥ 0</strong>. Mean = 1/λ, Variance = 1/λ<sup>2</sup>. This distribution displays</p>
<p><strong>Memorylessness</strong> - The property that the probability that the RV is greater than t + s given that it is greater than s equals the probability that the RV is greater than t (for positive s, t). This property is necessary for being able to model waiting times for Poisson processes.</p>
<p><strong>Normal/Gaussian distribution</strong> - The bell curve, the limit of the central limit theorem, describes natural processes that depend on many uncorrelated variables. RV with this pdf is a <strong>normal RV</strong>. Mean μ, Variance = σ<sup>2</sup></p>
<figure class="image image_resized image-style-block-align-left" style="width:75%;"><img style="aspect-ratio:640/62;" src="https://i.ibb.co/TmZrwkJ/image.png" width="640" height="62"></figure>
<p><strong>Lognormal distribution</strong> -&nbsp;</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:507/71;" src="https://i.ibb.co/P4GLHVQ/image.png" width="507" height="71"></figure>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:505/52;" src="https://i.ibb.co/GsGtY7V/image.png" width="505" height="52"></figure>
<h3>2.9 Distributions that are neither discrete nor continuous</h3>
<p>It is fairly simple to construct distributions with discontinuities. When this is the case, to calculate moments, you integrate x<sup>k</sup> with the CDF as the variable of integration using the <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Riemann%E2%80%93Stieltjes_integral">Riemann-Steiljes</a> integral.</p>
<h3>2.10 Limit Theorems</h3>
<p><strong>Law of large numbers</strong> - for a sequence of i.i.d. variables, the sample mean will approach the expectation value (the population mean) as n goes to infinity.</p>
<p><strong>Central limit theorem</strong> - for a sequence of i.i.d. variables, mean μ and variance σ<sup>2</sup>, then the probability approaches normal (known as <strong>asymptotic normality</strong>).</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:512/187;" src="https://i.ibb.co/zPYDw6j/image.png" width="512" height="187"></figure>
<h3>2.11 R applications</h3>
<p>There is a lot of good example code in this section that shows you how to graph different distributions. No use repeating it here, since there's nothing to summarize.</p>
<h2>Statistical Inference</h2>
<h3>3.1 The astronomical context</h3>
<p>When trying to learn about the underlying population, it is often possible to measure the properties of only a small sample of the full population of objects (often including bias). The inferences made are often based on a</p>
<p><strong>Statistic</strong> - a function of random variables (mean, median, mode, standard deviation, etc. though they can of course become very complicated)</p>
<p>Inference is done basically any time a conclusion is drawn from astronomical data. Most of the rest of the book are based on different inference techniques. This chapter gives the foundations.</p>
<h3>3.2 Concepts of statistical inference</h3>
<p><strong>Statistical inference</strong> - the very broad category of methods to draw conclusions about underlying populations from observed samples (including intrinsic uncertainties). Can be used for estimation or to test hypotheses. Can be parametric (requiring assumptions about underlying structure of population, like linear regression), nonparametric (don't assume model structure), or semi-parametric (combine aspects of both, e.g. local regression).</p>
<p><strong>Point estimation</strong> - The method of estimating the model parameters based on observations when the shape of the pdf of the underlying population is known (finding the mean and standard deviation of points drawn from a normal distribution, for example).</p>
<p><strong>Maximum likelihood estimation</strong> - The <strong>likelihood</strong> is the hypothesized probability distribution that past events were drawn from. It differs from probability, since probability relates to future unknown events, while likelihood is an unknown distribution from known past events. MLE entails taking the maximum of the likelihood as the estimator for the statistic.</p>
<p><strong>Confidence intervals</strong> - An x% confidence interval is likely to contain the estimated parameter with a probability of x%.</p>
<p><strong>Resampling methods</strong> - point estimation methods are often inherently variable. This is necessary if one wants to determine confidence intervals, for example. Sometimes the variance is not possible to express easily, or at all. Resampling methods construct hypothetical populations from existing observations and then examine all of them simultaneously to determine intrinsic variations. Bootstrapping is one of the most common and powerful example of these techniques.</p>
<p><strong>Testing hypotheses</strong> - instead of estimating parameters, the goal is to test if data are consistent with a hypothesis. Null hypothesis (the claim that the studied effect does not exist or that there is no relationship between datasets or variables) and alternative hypothesis. Result is either reject or not reject null hypothesis, which is <i>not the same as saying the null hypothesis is correct</i>. These types of tests can lead to <strong>false positives and false negatives</strong>, where the null hypothesis is wrongly rejected and wrongly failed to be rejected, respectively.</p>
<p><strong>Bayesian inference</strong> - observational evidence is used to infer or update inferences. The more measurements, the belief in any given model is likely to change. Encapsulated in the <strong>prior</strong>, the distribution that represents in a function all previous knowledge about the problem.</p>
<h3>3.3 Principles of point estimation</h3>
<p><strong>Model misspecification</strong> - Care must be taken that the chosen model is a good fit for the chosen population or physical process.</p>
<p><strong>Model validation</strong> - test of goodness-of-fit</p>
<p>Multiple ways to estimate best-fit parameters, including method of moments, least squares, and MLE. The correct choice will depend on the specific problem, but there are a lot of situations where it is possible to find the best-fit that are simultaneously unbiased, have minimum variance, and have maximum likelihood.</p>
<p>The point estimator of the true parameters is usually represented by theta hat and is a function of the random variables being explored. The estimator is calculated from a particular sample drawn from the population being estimated.</p>
<p>However, it is not always possible to optimize all of the important properties of the estimator. Some of the important criteria of a point estimator are:</p>
<p><strong>Unbiasedness</strong> - The bias of an estimator is the difference between the mean of an estimated parameter and its true value. <i>This is an intrinsic offest in the estimator, not the error of one representation of the estimator from a particular dataset.</i> <strong>Unbiased </strong>estimators have 0 bias, while <strong>asymptotically unbiased </strong>estimators have their bias approach zero as the number of datapoints goes to infinity.</p>
<p><strong>Mean square error</strong> - The sum of the variance and the square of the bias. It is the expectation value of the estimator minus the true value, quantity squared. Used in the evaluation of estimated parameters.</p>
<p><strong>Minimum variance unbiased estimator</strong> - term for the estimator with the lowest variance in a collection of unbiased estimators. It is usually considered the most desirable.</p>
<p><strong>Consistency</strong> - The trait of an estimator that approaches the true parameter as the sample size increases.</p>
<p><strong>Asymptotic normality</strong> - when an ensemble of consistent estimators approaches a gaussian distribution around the true value with a variance that decreases as 1/n.</p>
<h3>3.4 Techniques of point estimation</h3>
<p>Many probability distributions and models only depend on a few parameters. There are many techniques that one can use to obtain estimates of these parameters: The most common are method of moments, least squares, and maximum likelihood estimation. For a gaussian, for example, the mean and variance are estimated by the sample mean and sample variance, respectively.</p>
<p><strong>Method of moments</strong> - Moments describe basic parameters of a distribution (central location, width, asymmetries).The kth sample moment is basically a discrete version of the kth moment. You sum up each sample value to the kth power and divide by the sample size. Any parameter that can be expressed as simple function of the moments can be estimated in this way.</p>
<p><strong>Method of least squares</strong> - A significant application of least-squares is in regression (Section 7.3). The least-squares estimator of a parameter is obtained by minimizing the sum of the squares of the differences between the data and the population parameter being estimated. Weighted least squares is similar, but used for data with heteroscedastic errors. You instead minimize the weighted sum of squares, weighted by some known linear combination of parameters. This is related to <strong>Minimum χ<sup>2</sup> regression</strong>.</p>
<p><strong>Maximum likelihood method</strong> - Focuses on methods that give the most probable outcome for an estimator. The likelihood is the pdf viewed as a function of the data given model parameters with specific values. f(_;θ) is the pdf with parameter θ. Therefore, the likelihood and loglikelihood are given by:</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="q3FCW7wevhKL">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/720.webp 720w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/740.webp 740w" type="image/webp" sizes="(max-width: 740px) 100vw, 740px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/q3FCW7wevhKL/images/740.png" width="740" height="357">
    </picture>
</figure>
<p>where Xi are i.i.d. random variables, and you replace X with x for observed data. loglikelihood is usually computationally easier to calculate. The MLE estimator is usually unbiased, but if it isn't, this can usually be fixed by multiplying the estimator by a constant. For “nice” functions g of the parameter, the MLE of g is the MLE of the estimator. Many common situations have the MLE estimators as asymptotically normal (this is helpful for calculating confidence intervals).</p>
<p><strong>Coverage probability</strong> - The probability that the true parameter is within the x% confidence interval is at least x%. For 95% confidence interval, if the experiment were repeated 100 times, an average of 95 intervals will contain the true parameter value.</p>
<p>MLE confidence intervals depend on the variance and sample size. The 100(1-a)% confidence interval for the mean is:</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="Zd2nHZtsDJ7a">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/720.webp 720w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/749.webp 749w" type="image/webp" sizes="(max-width: 749px) 100vw, 749px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/Zd2nHZtsDJ7a/images/749.png" width="749" height="309">
    </picture>
</figure>
<p>where z and t are defined such that P(Z &gt; z) = P(T<sub>m</sub> &gt; t(m)) = a, where Z is the standard normal and T<sub>m</sub> is a t-distribution with m degrees of freedom.</p>
<p>Other confidence interval calculations can quickly get even more complicated, but thankfully statistics software is good at calculating them.</p>
<p><strong>Two-sided and One-sided</strong> - Two-sided confidence limits are calculated when the question permits higher and lower values. One-sided is asymmetrical.</p>
<p>Any numerical optimization method can be used to maximize likelihoods. The simplest analytical model is to simply set the derivative of the likelihood to 0 with respect to each parameter. This gives a system of p equations in p unknowns that can be solved with algebra.</p>
<p>There have been computational methods with varying degrees of effectiveness, but one of the most influential is the</p>
<p><strong>EM algorithm</strong> - considers mapping set of datasets to unknown complete set. The algorithm starts with initial values of the model parameters. The algorithm is iterative and has two steps: the <strong>expectation step</strong> calculates the likelihood for the current values of the parameter, and the <strong>maximization step</strong> updates the missing data values, assuring that likelihood with respect to the current model is maximized. This new dataset replaces the original and the algorithm is repeated until convergence. This is useful since each step guarantees an increase of the likelihood from the previous iteration.</p>
<h3>3.5 Hypothesis testing techniques</h3>
<p>Two competing statements are determined for a given experiment. The null and alternative hypotheses. It is impossible to simultaneously minimize false positives and negatives, so the scientist must decide what is more important for the specific question. Most commonly, let false negatives be uncontrolled, and keep false positives down to the 5% confidence level.</p>
<p><strong>Power</strong> - the probability of correctly rejecting the null hypothesis (i.e. when the alternative hypothesis is true) = 1 - false negative rate. Often want to look for the</p>
<p><strong>Uniformly most powerful</strong> - describes the test statistic that gives the highest power for all parameters at a chosen significance level.</p>
<p><strong>Statistically significant</strong> - the result of a test that is unlikely to have occurred by chance. Significant at the x level if you have 100(1-x)% confidence. Quantified also by the</p>
<p><strong>p-value</strong> - the probability of getting a value as extreme as the test statistic, <i>assuming the null hypothesis is true</i>.</p>
<p><i>The null hypothesis can never be accepted, it can only (fail to) be rejected at a given confidence level.</i> If many hypothesis tests are conducted on the same dataset (searching every pixel in an image for faint sources), significance levels must change. This is because there then must be a balance between false positives and sensitivity.</p>
<p><strong>False detection rate</strong> - A new procedure for combining multiple hypothesis tests that can control false positives.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="yAUIEUG5BgkU">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/656.webp 656w" type="image/webp" sizes="(max-width: 656px) 100vw, 656px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/yAUIEUG5BgkU/images/656.png" width="656" height="843">
    </picture>
</figure>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="n_eFW-7O9RMA">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/n_eFW-7O9RMA/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/n_eFW-7O9RMA/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/n_eFW-7O9RMA/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/n_eFW-7O9RMA/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/n_eFW-7O9RMA/images/364.webp 364w" type="image/webp" sizes="(max-width: 364px) 100vw, 364px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/n_eFW-7O9RMA/images/364.png" width="364" height="864">
    </picture>
    <figcaption>A useful summary of hypotheses and critical regions. They contain inequalities involving values of statistics computed from the data.</figcaption>
</figure>
<p><strong>Critical regions (rejection regions)</strong> - If a statistic falls in the critical region, the null hypothesis should be rejected.</p>
<p>In the above tables X bar is the sample mean of n i.i.d. random variables, and Y bar is the sample mean of m i.i.d. random variables from a population independent of x. y are observations from binomial distribution with population portion p with n trials. If n and m are small, then different critical regions will have different definitions.</p>
<p><strong>The t distribution</strong> - with v degrees of freedom. It is the sampling distribution for random samples from normal populations. <i>If x bar and s<sup>2</sup> are the mean and variance of a random sample of size n from a normal population, then (xbar - mu)/(s/sqrt(n)) has the t distribution with n-1 degrees of freedom.</i></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="6WhTPZ6b2mDB">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/720.webp 720w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/749.webp 749w" type="image/webp" sizes="(max-width: 749px) 100vw, 749px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/6WhTPZ6b2mDB/images/749.png" width="749" height="183">
    </picture>
</figure>
<h3>3.6 Resampling methods</h3>
<p>Finding the distribution of any given statistic is often difficult in general. However, this limitation can be somewhat overcome with</p>
<p><strong>Resampling methods</strong> - statistical methods that involve building synthetic populations from limited observations that can all be analyzed in the same way to see how statistics depend on random variations in the observations. Resampling methods do well to preserve the true distributions in the underlying real population, including things like truncation and censoring.</p>
<p><strong>Half-sample method</strong> - randomly choose half the data points, calculate the statistic, repeat. Estimation of the statistic can then be based on a histogram of the resampled statistics. Also called <strong>interpenetrating samples</strong>. One very popular variant of this is the</p>
<p><strong>Jackknife method</strong> - For a dataset with n points, calculate n datasets ("jackknife samples") with n-1 points, each omitting a different point, the calculate the statistic for each sample. Helps with detecting variation with n (bias), as in general, we want n = infinity. Can fix bias of order 1/n, but not greater.</p>
<p><strong>Bootstrap</strong> - Resampling with replacement. Create a large number of datasets, each with n points randomly drawn from the original data. This means that the simulated datasets will each miss some points and also and multiple copies of the same data point. In that way, it is a Monte Carlo method to simulate from existing data without any assumption of the underlying population &nbsp;(i.e. it is <strong>nonparametric</strong>). Useful for generating confidence intervals, as well.</p>
<h3>3.7 Model selection and goodness-of-fit</h3>
<p>There are a lot of situations where the “best fit” of a particular model to data is required. The overall procedure for doing so is:</p>
<ol>
    <li>Choose a model or family of models based on existing knowledge</li>
    <li>Obtain best-fit parameters using point estimation techniques from section 3.3</li>
    <li>See if the model agrees with the data at the selected significance level (using hypothesis tests)</li>
    <li>Repeat with alternative models and vary to select the best model based on some criteria.</li>
</ol>
<p>The choice of model is outside the scope of the book and depends on the situation, but one should normally choose the simplest model that is consistent with the data.</p>
<p><strong>Nested/nonnested models</strong> - nested models refer to the case where the simpler model is a subset of the more elaborate model(s) in question. These are easier to compare with one another.</p>
<p><strong>Reduced χ<sup>2</sup> statistic</strong> - To calculate, the model is compared to binned data and best-fit parameters are obtained by weighted least squares, and the weights are obtained from the dataset (like the standard deviation equaling the square root of the number of data points in the bin) or ancillary measurements (e.g. noise level in background of a CCD image).</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="yEkAm-4910Fa">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/720.webp 720w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/750.webp 750w" type="image/webp" sizes="(max-width: 750px) 100vw, 750px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/yEkAm-4910Fa/images/750.png" width="750" height="126">
    </picture>
    <figcaption>Mathematical representation of this process.</figcaption>
</figure>
<p>Doing the above gets you the <strong>minimum χ<sup>2</sup>&nbsp;parameters</strong>. Goodness of fit is chosen to be when <strong>χ<sup>2</sup><sub>v</sub></strong>, where v is the number of degrees of freedom.</p>
<p>There are three main categories of nonparametric hypothesis tests, based on the KS, Cramer-von Mises, and AD statistics, and they are used to compare empirical distributions of univariate data to the hypothesized distribution function of the model. These are described in more detail in Section 5.3. It is important to state, however, that there is a distinction between the KS statistic and the KS test. Also, the pre-tabulated probabilities are only valid for models where all the parameters are estimated independently, which is uncommon in astronomy. This problem can be alleviated with bootstrapping.</p>
<p><strong>Parametric bootstrap</strong> - simulated datasets are obtained from the best-fit model. Using this to get 90 to 99% critical values replaces the standard probability tabulation of the KS test.</p>
<p>Likelihood-based methods for model selection use hypothesis testing based on maximum likelihood estimators. Below are some examples:</p>
<p><strong>Wald test</strong> - provides a test for the null hypothesis that the value of the parameter equals theta naught. It represents the standardized distance between theta naught and the MLE theta hat.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="yVSDr6Tlp28r">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/yVSDr6Tlp28r/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yVSDr6Tlp28r/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yVSDr6Tlp28r/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yVSDr6Tlp28r/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yVSDr6Tlp28r/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/yVSDr6Tlp28r/images/459.webp 459w" type="image/webp" sizes="(max-width: 459px) 100vw, 459px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/yVSDr6Tlp28r/images/459.png" width="459" height="221">
    </picture>
</figure>
<p><strong>Likelihood ratio test</strong> - use the ratio of the likelihoods for theta hat and theta naught.</p>
<p><strong>Score test</strong> - uses this statistic.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="3Gdm6ZpqFPzd">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/3Gdm6ZpqFPzd/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/3Gdm6ZpqFPzd/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/3Gdm6ZpqFPzd/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/3Gdm6ZpqFPzd/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/3Gdm6ZpqFPzd/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/3Gdm6ZpqFPzd/images/467.webp 467w" type="image/webp" sizes="(max-width: 467px) 100vw, 467px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/3Gdm6ZpqFPzd/images/467.png" width="467" height="213">
    </picture>
</figure>
<p>where l' is the derivative of the loglikelihood, and I is Fisher's information:</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="KrTcs9s6RzfM">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/685.webp 685w" type="image/webp" sizes="(max-width: 685px) 100vw, 685px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/KrTcs9s6RzfM/images/685.png" width="685" height="157">
    </picture>
</figure>
<p>where X is a random variable and theta is a parameter.</p>
<p>Sometimes it is better to take an alternative approach with</p>
<p><strong>Penalized likelihoods</strong> - when using nested models, the largest likelihood in the nested model is always smaller because it has fewer parameters. So, you can apply a penalty to compensate for this difference.</p>
<p><strong>Akaike information criterion</strong> - defined for model j as</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="5x-pnOf4i3LV">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/564.webp 564w" type="image/webp" sizes="(max-width: 564px) 100vw, 564px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/5x-pnOf4i3LV/images/564.png" width="564" height="102">
    </picture>
</figure>
<p>where p is the number of parameters in the jth model. The first term is goodness-of-fit, and the 2p is the penalty term. This criterion finds the model that maximizes the AIC. Another common criterion is the</p>
<p><strong>Bayesian information criterion</strong> - where the penalty comes from the parameters and the number of data points</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="SDiGJG1XfOSn">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/600.webp 600w" type="image/webp" sizes="(max-width: 600px) 100vw, 600px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/SDiGJG1XfOSn/images/600.png" width="600" height="103">
    </picture>
</figure>
<h3>3.8 Bayesian statistical inference</h3>
<p><strong>Posterior probability</strong> - the conditional probability of the model given the data. This is the result of the calculation</p>
<p><strong>Likelihood function</strong> - The conditional probability of the data given the model. The assumed distribution the data are drawn from.</p>
<p><strong>Prior information</strong> - The marginal probability of the model. There is no information from the data, but this is where prior information of the problem is included.</p>
<p>The posterior of a chosen set of models given the observable is equal to the product <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">of the likelihood of the data for those models and the prior probability on the models (normalized).</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Bayesian inference</strong> - applying the steps above (Baye's theorem) to assess the degree to which a dataset is consistent with a hypothesis.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Bayesian inference works best when there is </span><i><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">prior </span></i><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">knowledge to get a realistic </span><i><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">prior</span></i><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">. Where MLE gives the maximum of the likelihood, Bayesian first weights this likelihood by the prior. Some useful prior distributions include:</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>beta distribution</strong> - flexible choice for a prior</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="UaFaVeZBeDV6">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/697.webp 697w" type="image/webp" sizes="(max-width: 697px) 100vw, 697px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/UaFaVeZBeDV6/images/697.png" width="697" height="174">
    </picture>
</figure>
<p><strong>Maximum a posterior/highest posterior density estimate</strong> - MLE of the posterior.</p>
<p><strong>Normal mixture model</strong> - unknown number of normal components</p>
<h3>R applications - none here, included in later chapters</h3>
<h2>Probability Distribution Functions</h2>
<p>This chapter focuses on a few pdfs that are useful to astronomy. For any information not in the table below, look to Wikipedia, since these are all common distributions.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="mDhl7Led-M0L">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/86.webp 86w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/172.webp 172w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/258.webp 258w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/344.webp 344w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/430.webp 430w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/516.webp 516w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/602.webp 602w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/688.webp 688w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/774.webp 774w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/855.webp 855w" type="image/webp" sizes="(max-width: 855px) 100vw, 855px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/mDhl7Led-M0L/images/855.jpeg" width="855" height="402">
    </picture>
</figure>
<h3>4.1 Binomial and multinomial</h3>
<p>Described above in chapter 2, it describes probabilities of outcomes from independent binary trials. The multinomial is just the k-dim generalization (so for k possible outcomes). The MLE, minimum chi square, and MVUE are all p = X/n. p is the probability of success occurring in the population, X is the number of successes.</p>
<p>Many problems require the estimation of the ratio of binomial random variables (e.g. SNR, hardness ratio in x-ray, etc.). These are harder to deal with when n is small. For small datasets (n&lt;40), it is better to use the Wilson estimator. Below is it for the 95% confidence interval</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="vkNiHP5B5UYP">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/705.webp 705w" type="image/webp" sizes="(max-width: 705px) 100vw, 705px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/vkNiHP5B5UYP/images/705.png" width="705" height="210">
    </picture>
</figure>
<h3>4.2 Poisson</h3>
<p>Events follow the Poisson distribution if they are produced by Bernouilli trials where p is very small, but n is large, so np approaches a constant. This occurs naturally in photon counting, radioactive decay, etc. Successive probabilities are given by λ/(x+1).</p>
<p><strong>Nonhomogeneous/nonstationary Poisson process</strong> - when the counting process is mixed with another pdf (e.g. the intensity changes with time)</p>
<p><strong>Marked Poisson processes</strong> - association of other RVs (e.g. mass) to a Poisson variable (e.g. position in space). The distribution of these variables is also Poisson.</p>
<p>There are also <strong>truncated and censored Poisson distributions</strong></p>
<p><strong>Mixed Poisson processes</strong> - the intensity itself is the outcome of a separate RV with some known or assumed structure.</p>
<h3>4.3 Normal and lognormal</h3>
<p>The sample mean is simultaneously the least-squares estimator and MLE. A normal RV can be standardized, as explained above. Should still use t distribution for confidence intervals.</p>
<p>It is often necessary to evaluate whether a particular statistic is consistent with the normal distribution (for asymptotically normal situations). AD, CvM and KS tests are used. The <strong>Shapiro-Wilks test</strong> is also used for this purpose, it uses rank and least-squares measurements.</p>
<p><strong>Multivariate normal distribution</strong> - a random vector where every linear combination of its components has a univariate normal distribution.</p>
<h3>4.4 Pareto (power-law)</h3>
<p><strong>Power-law/Pareto</strong> - used very frequently in astronomy. P(X &gt; x) is proportional to (x/x<sub>min</sub>)<sup>-a</sup>, a is the index of the power law.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="li9-shKh5wrw">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/663.webp 663w" type="image/webp" sizes="(max-width: 663px) 100vw, 663px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/li9-shKh5wrw/images/663.png" width="663" height="223">
    </picture>
</figure>
<p>where α + 1 is the <strong>power-law slope.</strong></p>
<p>The least-squares estimator of the power law slope has significant bias when using binned data. Some of these problems are resolved with MLE, but the estimator is not equal to unbinned MLE. So, in short, <i>avoid binned data when estimating power-law parameters</i>.</p>
<p>ML techniques for power law are (asymptotically) unbiased, normal, and minimum variance. The likelihood is simply the product of P (above) for every data point. These lead to the following estimators:</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="YjAcJWZprSnI">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/671.webp 671w" type="image/webp" sizes="(max-width: 671px) 100vw, 671px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/YjAcJWZprSnI/images/671.png" width="671" height="452">
    </picture>
</figure>
<p>where X<sub>(1)</sub> is the smallest X value.</p>
<p>There are many more complicated distributions that equal Pareto under specific circumstances. In astronomy, the most common generalization is the</p>
<p><strong>Double Pareto/broken power-law</strong> - power law with two different slopes, changes at a <strong>break point</strong>. Like the Kroupa IMF.</p>
<h3>4.5 Gamma</h3>
<p><strong>Gamma distribution</strong> - A generalization of the Pareto distribution: power law at low x and exponential at high x. It is linked with the Poisson distribution: its CDF gives the probability of observing <i>a</i> or more events. It also results from t<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">he sum of i.i.d. exponential random variables.</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="XJeyR9AVGKfl">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/XJeyR9AVGKfl/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/XJeyR9AVGKfl/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/XJeyR9AVGKfl/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/XJeyR9AVGKfl/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/XJeyR9AVGKfl/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/XJeyR9AVGKfl/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/XJeyR9AVGKfl/images/555.webp 555w" type="image/webp" sizes="(max-width: 555px) 100vw, 555px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/XJeyR9AVGKfl/images/555.png" width="555" height="94">
    </picture>
</figure>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>χ<sup>2</sup><sub>v</sub> distribution</strong> - special case of the gamma distribution where b = 2 and α = ν/2.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">R applications</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">The book lists useful packages that contain different distributions. There are also examples on comparing Pareto estimators, fitting distributions to data, tests for normality, and tests for multimodality.</span></p>
<h2><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Nonparametric statistics</span></h2>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">5.1 The astronomical context</span></h3>
<p>In astronomy, the underlying phenomena are almost always more complex than the simple models provided in the book, so choosing such a model can obfuscate characteristics of the data. Therefore it is clear why nonparametric statistical tools are needed to avoid making assumptions.</p>
<h3>5.2 Concepts of nonparametric inference</h3>
<p><strong>Nonparametric statistics</strong> - Two categories: 1) procedures that do not involve or depend on parametric assumptions regardless of whether or not the underlying population belongs to a parametric family, and 2) methods that do not require data belong to a particular parametric family.</p>
<p><strong>Semi-parametric</strong> - combination of parametric modeling with nonparametric principles.</p>
<p>Sometimes nonparametric procedures have parameters, but they are</p>
<p><strong>Robust</strong> - insensitive to slight deviations from model assumptions (until the <strong>breakdown point</strong>).</p>
<p>Some procedures are parametric but operate on ranks instead of values.</p>
<h3>5.3 Univariate problems</h3>
<p><strong>Empirical distribution function</strong> - simplest and most direct (unbiased and consistent) nonparametric estimator of CDF. Made up of steps with heights of 1/n located at the values of Xi.</p>
<p><strong>KS test</strong> - measures the maximum distance between the edf and the model. Used to test the null hypothesis that the edf equals the cdf of the proposed distribution. A large value allows rejection of the null hypothesis that the two distributions are equal. here is the cumulative distribution for the one-sample KS statistic. Mcrit is the critical value.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="mzFnljznWYEY">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/720.webp 720w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/725.webp 725w" type="image/webp" sizes="(max-width: 725px) 100vw, 725px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/mzFnljznWYEY/images/725.png" width="725" height="377">
    </picture>
</figure>
<p><i>The KS test has serious limitations:</i></p>
<ol>
    <li>KS is not distribution-free: pre-tabulated critical values of KS statistic are not valid if the model parameters were estimated from the data set being tested, only if the model parameters are known in advance.</li>
    <li>Also not distribution-free if the dataset has two or more dimensions because a unique ordering of points is necessary to construct an edf.</li>
    <li>KS is sensitive to differences between two edfs or one edf and the cdf having different means.</li>
    <li>Less efficient efficient near tails of distribution</li>
</ol>
<p><strong>CvM statistic</strong> - measures the sum of the squared differences between the edf and cdf. Captures global and local differences, so often performs better than KS. Similar limitation to KS, again, in that the edf and cdf converge to 0 and 1 at the ends, so differences there are squeezed.</p>
<p>Two-sample tests compare two edfs versus an edf and cdf.</p>
<p><strong>AD statistic</strong> - provides consistent sensitivity across the full range of x. It is a weighted variant of the CvM statistic. <i>Usually the best choice of these 3.</i></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="aWZ2EuWlyGXC">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/720.webp 720w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/728.webp 728w" type="image/webp" sizes="(max-width: 728px) 100vw, 728px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/aWZ2EuWlyGXC/images/728.png" width="728" height="170">
    </picture>
</figure>
<p><strong>Median</strong> - central value, very robust. mean is not robust. These two are limiting cases of</p>
<p><strong>trimmed means</strong> -&nbsp;</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="PKpLehSiDYKe">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/696.webp 696w" type="image/webp" sizes="(max-width: 696px) 100vw, 696px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/PKpLehSiDYKe/images/696.png" width="696" height="210">
    </picture>
</figure>
<p>where m = 0 gives untrimmed mean, and 0 &lt; m &lt; 0.5n trim a range of values from both ends of the distribution. A similar estimate of central location is the</p>
<p><strong>trimean</strong> - weighted mean of the 25%, 50%, and 75% quantiles.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="aTVAKyuEExrM">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/720.webp 720w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/731.webp 731w" type="image/webp" sizes="(max-width: 731px) 100vw, 731px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/aTVAKyuEExrM/images/731.png" width="731" height="133">
    </picture>
</figure>
<p><strong>Interquartile range</strong> - the distance between the 25% and 75% quantiles.</p>
<p>The book suggests, when measuring dispersion, to instead use the</p>
<p><strong>Median absolute deviation</strong> = Med|Xi - Med|. Very stable estimator of dispersion, but not very efficient or precise. The <strong>normalized MAD</strong> scales the MAD to the standard deviation of a normal distribution.</p>
<p><strong>M-estimators</strong> - robust statistics. For example, M-estimates of location are weighted means where outliers are downweighted.</p>
<h3>5.4 Hypothesis testing</h3>
<p>There are nonparametric hypothesis tests to test the central location of a distribution</p>
<p><strong>Sign test</strong> - simple test that compares the number of points above the assumed value for the median with the number of points below the assumed value. If it is the true median, then the statistic is 0.</p>
<p>There are many nonparametric methods for comparing two or more univariate datasets that test the null hypothesis that the two underlying populations are identical.</p>
<p><strong>MWW/Wilcoxon sum of rank test</strong> - collect all datapoints from the two sets and put them in ascending order. Then replace each point by 1, 2, 3, … in sequence (the ranks), awarding average ranks to ties.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="oFJTQt8QgldQ">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/oFJTQt8QgldQ/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/oFJTQt8QgldQ/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/oFJTQt8QgldQ/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/oFJTQt8QgldQ/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/oFJTQt8QgldQ/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/oFJTQt8QgldQ/images/436.webp 436w" type="image/webp" sizes="(max-width: 436px) 100vw, 436px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/oFJTQt8QgldQ/images/436.png" width="436" height="162">
    </picture>
</figure>
<p>is the test statistic, where R represents the rank, and the 1 represents the first sample. For m,n &gt; 7, the statistic is asymptotically normal, so the z statistic can be compared with this to see if it is significant.</p>
<p>MWW acts as a nonparametric substitute for the t test.</p>
<p>There are related tests that we didn't talk about in class in the book.</p>
<h3>5.5 Contingency tables</h3>
<p><strong>Contingency tables</strong> - table that presents data categorically rather than numerically. Can use <strong>χ<sup>2</sup></strong> test if looking for the independence between attributes.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="QKJolt6uy05m">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/700.webp 700w" type="image/webp" sizes="(max-width: 700px) 100vw, 700px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/QKJolt6uy05m/images/700.png" width="700" height="211">
    </picture>
</figure>
<p>for a generalized r by c contingency table, where O is the observed number and E is the expected. The null distribution is the χ<sup>2</sup> distribution with (r-1)(c-1) degrees of freedom.</p>
<p><strong>Fisher's Exact Test</strong> - Determines probability of getting the observed results if hypothesis that the two parameters in the 2 by 2 contingency table are unrelated. Turns into a hypergeometric distribution (sampling finite population without replacement)</p>
<h3>5.6 Bivariate and multivariate tests</h3>
<p><strong>Spearman's rank test</strong> - for the independence between two variables. Subtract to find the difference between the rank values for each pair of observations. Square each difference and add them all up. Ties require a correction.</p>
<p><strong>Kendall tau correlation measure</strong> - usually nearly identical to Spearman, but is sometimes preferable as it approaches normality factor for small samples. The ratio of the difference of concordant and discordant pairs to the sum of concordant and discordant pairs.</p>
<h3>5.7 Remarks</h3>
<p>Nonparametrics should be used more often because the underlying physics are never perfectly understood.</p>
<h3>R applications</h3>
<p>The examples create summary plots and statistics and simple visualizations of a univariate dataset. They show a boxplot and display different test for normality. They create empirical distributions, show the quantile functions as well as Q-Q plots. There are also examples of two-sample tests and an example of contingency tables.</p>
<h2>Data smoothing</h2>
<h3>6.1 astronomical context</h3>
<p><strong>Density estimation</strong> - estimate unknown pdf of RV from a set of observations, or <i>turn individual measurements into a continuous distribution</i>.</p>
<h3>6.2 Concepts of density estimation</h3>
<p><strong>Nonparametric density estimation</strong> - when there is no assumption of parametric form</p>
<h3>6.3 Histograms</h3>
<p>Histograms are nearly ubiquitous in astronomy, but they have a lot of downsides.</p>
<p><strong>Histogram estimator</strong> - The number of points in the m-th bin containing x, where h is the bin width for the bin that x is in.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="eQ2Oid-7ar3R">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/eQ2Oid-7ar3R/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eQ2Oid-7ar3R/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eQ2Oid-7ar3R/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eQ2Oid-7ar3R/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eQ2Oid-7ar3R/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eQ2Oid-7ar3R/images/456.webp 456w" type="image/webp" sizes="(max-width: 456px) 100vw, 456px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/eQ2Oid-7ar3R/images/456.png" width="456" height="200">
    </picture>
</figure>
<p>The choice of bin width is the most important decision when creating a histogram. There are many different choices for binwidth.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="kAfWYvu5fEHS">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/711.webp 711w" type="image/webp" sizes="(max-width: 711px) 100vw, 711px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/kAfWYvu5fEHS/images/711.png" width="711" height="180">
    </picture>
</figure>
<p>is used when the population resembles a normal distribution, where ⅓ is replaced by 1/(2 + p) for multivariate datasets with p dimensions. The MSE has a bias that shrinks with shrinking h, and a variance that decreases for increasing h. Other choices of bin width work to balance the tension between the two.</p>
<p>Histograms have <i>several problems</i>:</p>
<ol>
    <li>No guidance for choosing the origin for the histogram. Can be mitigated with <strong>average shifted histograms</strong></li>
    <li>No guidance for choosing the method for grouping data into bins: equal spacing but unequal points, unequal spacing but equal number of points, or something else.</li>
    <li>If being used for further analysis, a central bin location must be specified, but this causes a bias.</li>
    <li>The choice of optimal bin width requires knowing the true distribution, finding which is the point of the histogram in the first place</li>
    <li>Multivariate histograms are difficult to visualize or interpret</li>
    <li>Discontinuities between bins don't accurately reflect continuous behavior</li>
</ol>
<p>Even though there are all these drawbacks, <i>histograms are appropriate when the original data is binned</i></p>
<h3>6.4 Kernel density estimators</h3>
<p>Convolve discrete data with a kernel to create a smooth density. However, there are many possible choices for the functional form and smoothing parameter/bandwidth for the kernel.</p>
<p>A constant bandwidth <strong>kernel estimator</strong> for i.i.d. random variables has the form</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="4WZMrss6ruUv">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/703.webp 703w" type="image/webp" sizes="(max-width: 703px) 100vw, 703px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/4WZMrss6ruUv/images/703.png" width="703" height="157">
    </picture>
</figure>
<p>where K is the normalized kernel function.</p>
<p><strong>Mean integrated square error</strong> - criterion for applicability of a KDE to an underlying pdf.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="wttwwhFdykrf">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/653.webp 653w" type="image/webp" sizes="(max-width: 653px) 100vw, 653px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/wttwwhFdykrf/images/653.png" width="653" height="96">
    </picture>
</figure>
<p>where E is the mean. It is also the sum of bias and variance terms, so the optimal bandwidth is chosen to minimize the MISE.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="WKCUritBMPti">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/WKCUritBMPti/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/WKCUritBMPti/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/WKCUritBMPti/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/WKCUritBMPti/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/WKCUritBMPti/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/WKCUritBMPti/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/WKCUritBMPti/images/527.webp 527w" type="image/webp" sizes="(max-width: 527px) 100vw, 527px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/WKCUritBMPti/images/527.png" width="527" height="204">
    </picture>
</figure>
<p>the Gaussian density with mean 0 and variance 1 is a very common choice of kernel, though it is not optimal for minimizing MISE. Other choices include the</p>
<p><strong>Epanechikov kernel</strong> - an inverted parabola kernel over -1 to 1</p>
<p>and an inverted triangle. The boxcar kernel is a rectangle, though it has weaker performance.</p>
<p>Finding the correct bandwidth is very important to minimizing the MISE, and there are many difference possible choices.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="zvXuuNZfvL_s">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/zvXuuNZfvL_s/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/zvXuuNZfvL_s/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/zvXuuNZfvL_s/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/zvXuuNZfvL_s/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/zvXuuNZfvL_s/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/zvXuuNZfvL_s/images/426.webp 426w" type="image/webp" sizes="(max-width: 426px) 100vw, 426px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/zvXuuNZfvL_s/images/426.png" width="426" height="117">
    </picture>
</figure>
<p>where A is the minimum of the standard deviation and interquartile range is a rule of thumb bandwidth, though it doesn't work for multimodal distributions.</p>
<p>It is, in general, very difficult to minimize the MISE by direct calculation. A way around this is with</p>
<p><strong>Cross-validation</strong> - jackknife the dataset and calculate the KDE for each one. The average loglikelihood over these datasets is&nbsp;</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="eDaaxxJU1YiE">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/643.webp 643w" type="image/webp" sizes="(max-width: 643px) 100vw, 643px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/eDaaxxJU1YiE/images/643.png" width="643" height="175">
    </picture>
</figure>
<p>and the optimal bandwidth maximizes CV. There is a least-squares version that is resistant to outliers.</p>
<p>In multiple dimensions, the estimator can be a single multivariate density function or the product of univariate kernels.</p>
<h3>6.5 Adaptive smoothing</h3>
<p>There are oftentimes datasets with multiple peaks or detailed structure over a range of scales. In this case, the previous KDEs lose underlying information.</p>
<p>There are multiple different approaches one can use. For example, a particularly simple one is&nbsp;</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="Q4TUtHqZQXxH">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/Q4TUtHqZQXxH/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Q4TUtHqZQXxH/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Q4TUtHqZQXxH/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Q4TUtHqZQXxH/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Q4TUtHqZQXxH/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Q4TUtHqZQXxH/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/Q4TUtHqZQXxH/images/508.webp 508w" type="image/webp" sizes="(max-width: 508px) 100vw, 508px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/Q4TUtHqZQXxH/images/508.png" width="508" height="193">
    </picture>
</figure>
<p>where h is the global optimal bandwidth and f is an approximation of the pdf.</p>
<p>A more complicated but widely applicable approach starts with a first estimate of the density f~ using a standard kernel and the optimal constant bandwidth. Then, λ, the local bandwidth factor is obtained at each x point</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="buxP9OMHie5u">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/656.webp 656w" type="image/webp" sizes="(max-width: 656px) 100vw, 656px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/buxP9OMHie5u/images/656.png" width="656" height="276">
    </picture>
</figure>
<p>where g is the geometric mean of the first estimator and α is a sensitivity parameter between 0 and 1 (how sensitive the final kernel is to local variations). Then, finally, the adaptive estimator at x is given by&nbsp;</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="eK1qQm-GhbRM">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/715.webp 715w" type="image/webp" sizes="(max-width: 715px) 100vw, 715px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/eK1qQm-GhbRM/images/715.png" width="715" height="136">
    </picture>
</figure>
<p><strong>k-th nearest neighbor</strong> - Exactly what it sounds like. The density estimator based on it is given by</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="tbUH2OJ5yBYI">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/683.webp 683w" type="image/webp" sizes="(max-width: 683px) 100vw, 683px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/tbUH2OJ5yBYI/images/683.png" width="683" height="202">
    </picture>
</figure>
<p>where p is the dimensionality. It is useful in datamining, but it has disadvantages: it is not a pdf (not normalized), it is discontinuous, prone to local noise, no general best choice of k.</p>
<h3>6.6 Nonparametric regression</h3>
<p>Many datasets are multivariate tables of properties. Many scientific questions are about quantifying <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">the dependence of a <strong>response variable</strong> (dependent variable) y on one or more <strong>independent variables</strong> x. Here we look at methods that do not guess the true parametric functional relationship. It should be used when the physical processes linking y and x are not known. These are not common in astronomy, </span><i><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">but should be moreso.</span></i></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>NW estimator</strong> - For i.i.d. bivariate data, the estimated regression function given by</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="OUP011MHo_R9">
    <picture>
        <source srcset="https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/80.webp 80w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/160.webp 160w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/240.webp 240w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/320.webp 320w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/400.webp 400w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/480.webp 480w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/560.webp 560w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/640.webp 640w,https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/688.webp 688w" type="image/webp" sizes="(max-width: 688px) 100vw, 688px"><img src="https://ckbox.cloud/8b0a10a6a410e7760900/assets/OUP011MHo_R9/images/688.png" width="688" height="285">
    </picture>
</figure>
<p>It is a weighted local average of the dependent variable. Also the least-squares regression fit of y as a function of x when the y values are averaged over a window of width 2h<sub>x</sub> centered at Xi. This concept has been generalized to fit higher order polynomials in these windows to more accurately model gradients and can be generalized to multivariate problems, such as with the <strong>LOWESS method </strong>that involves calculating kernels for local polynomial regression.</p>
<p><strong>Range</strong> - equivalent to local bandwidth</p>
<p><strong>Spline smoothing</strong> - A chosen function (usually a cubic) is fitted <i>locally</i> to data with a roughness penalty to constrain the smoothness. The curve is evaluated at <strong>knots</strong> (usually between points) rather than data points themselves. Smoothing splines have small bias, but don't necessarily minimize MISE.</p>
<p>There are multivariate extensions to this type of regression, including</p>
<p><strong>Kriging</strong> - Least-squares interpolation method for irregularly spaced data points (more on this later).</p>
<h3>R applications</h3>
<p>Here they give examples of histograms, quantile functions, and representing measurement errors. There are some examples of different kernel smoothers (both constant and adaptive) as well as how measurement errors impact these. They also show some examples of nonparametric regression like spline fits, NW estimators, local regression, and LOESS.</p>
<h2>Regression</h2>
<h3>7.1 Astronomical context</h3>
<p>Fitting models to data is extremely common in astronomy. Could be a simple line or a complex model.</p>
<h3>7.2 Concepts of regression</h3>
<p>Involves estimating functional relationships between dependent and independent variables. with random error. For example, the expectation of the response variable Y given the independent variable(s) X is a function of X, where the function depends <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">on the model parameters, plus noise. This is in general a multivariate problem, but this book focuses on bivariate regression (though the methods used are usually also applicable in higher dimensions). The goal of regression usually focuses on estimation of the parameters of the function and their confidence interavls. The independent variable is assumed to have no uncertainty, thought this is accounted fro with <strong>measurement error models</strong>.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Linear regression model</strong> -&nbsp;</span></p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="MMb_sM5TnON7">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/MMb_sM5TnON7/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/MMb_sM5TnON7/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/MMb_sM5TnON7/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/MMb_sM5TnON7/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/MMb_sM5TnON7/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/MMb_sM5TnON7/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/MMb_sM5TnON7/images/525.webp 525w" type="image/webp" sizes="(max-width: 525px) 100vw, 525px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/MMb_sM5TnON7/images/525.png" width="525" height="90">
    </picture>
</figure>
<p>where X is the independent variable, Y is the dependent variable, beta contains the unknown intercept and slope.</p>
<p><strong>Structural models</strong> - when the X's are random variables</p>
<p><strong>Functional models</strong> - when the X's are deterministic.</p>
<p><strong>Intrinsic scatter</strong> - exactly what it sounds like. Occurs when Y values do not fall exactly on the parametric curve. These are intrinsic errors in the response variable. When the Y values should lie precisely on the curve, the scatter is <strong>measurement error</strong>.</p>
<p>So far, we assumed X and Y are RVs that take on real values. However, many astronomical situations use regression on different data structures. X could be location in an image, or wavelength in a spectrum, or other fixed variables with predetermined locations.</p>
<p><strong>Poisson regression</strong> - the above situation and when Y is an integer variable with small values based on a counting process. (more on this later)</p>
<p><strong>Logistic regression</strong> - When Y is a binary variable with values of 0 or 1 (or a multivariate categorical variable) (more on this later)</p>
<p>Ordinary regression should only be used when the scientific problem naturally specifies a dependent variable. In statistics, “linear models” refer to any model that is linear in the parameters, not the model variables. This is opposed to just a straight line, per se.</p>
<p><strong>Mixtures</strong> - sums of linear models. They are also linear themselves.</p>
<p>Once a model is chosen, can use any of these previously desc<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">ribed parameter estimation techniques.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">7.3 Least-squares linear regression</span></h3>
<p><strong>Residual sum of squares</strong> - The sum of the squares of the observed value minus the model predictions.</p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Ordinary least-squares estimator</strong> - the estimator for the linear regression model that minimizes the RSS:</span></p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="P6FcDD85-W0P">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/P6FcDD85-W0P/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/P6FcDD85-W0P/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/P6FcDD85-W0P/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/P6FcDD85-W0P/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/P6FcDD85-W0P/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/P6FcDD85-W0P/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/P6FcDD85-W0P/images/536.webp 536w" type="image/webp" sizes="(max-width: 536px) 100vw, 536px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/P6FcDD85-W0P/images/536.png" width="536" height="110">
    </picture>
</figure>
<p>The estimates of beta are given below</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="73pvsB9mjUFJ">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/73pvsB9mjUFJ/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/73pvsB9mjUFJ/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/73pvsB9mjUFJ/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/73pvsB9mjUFJ/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/73pvsB9mjUFJ/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/73pvsB9mjUFJ/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/73pvsB9mjUFJ/images/539.webp 539w" type="image/webp" sizes="(max-width: 539px) 100vw, 539px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/73pvsB9mjUFJ/images/539.png" width="539" height="151">
    </picture>
</figure>
<p>where S<sub>XY</sub> represents the sum of squared residuals of X and Y.</p>
<p>The estimator of the variance is RSS/(n - 2). These are also the MLE</p>
<p><strong>Standard error</strong> - The sample standard deviation divided by S<sub>XX</sub></p>
<p>It is oftentimes best to use bootstrap to determine uncertainties.</p>
<p><strong>Paired bootrap</strong> - A random sample (<i>X</i>1,<i>Y</i>1), (<i>X</i>2,<i>Y</i>2),… ,is drawn from two populations (X1,Y1), (X2,Y2),… with replacement. The estimators are generated for each bootstrap (the same way as the equations above), and the variance is average sum of square residuals between the estimator and the bootstrapped estimator.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="oEjQIMJqx2nm">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/oEjQIMJqx2nm/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/oEjQIMJqx2nm/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/oEjQIMJqx2nm/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/oEjQIMJqx2nm/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/oEjQIMJqx2nm/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/oEjQIMJqx2nm/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/oEjQIMJqx2nm/images/529.webp 529w" type="image/webp" sizes="(max-width: 529px) 100vw, 529px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/oEjQIMJqx2nm/images/529.png" width="529" height="135">
    </picture>
</figure>
<p><strong>Classical bootstrap</strong> - Applied where resamples are obtained by random selection of the <i>r<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">esiduals </span></i><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">from the original model fit, rather than by random selection from the original data points.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>M-estimation</strong> - and example of a <strong>robust </strong>estimator that keeps all points but minimizes a function with less dependence on outlying points. For any function psi and any solution beta,</span></p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="6m7-FI16KfJT">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/6m7-FI16KfJT/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/6m7-FI16KfJT/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/6m7-FI16KfJT/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/6m7-FI16KfJT/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/6m7-FI16KfJT/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/6m7-FI16KfJT/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/6m7-FI16KfJT/images/522.webp 522w" type="image/webp" sizes="(max-width: 522px) 100vw, 522px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/6m7-FI16KfJT/images/522.png" width="522" height="185">
    </picture>
</figure>
<p>is an M-estimator. MLE and least squares are both examples of M-estimators.Asymptotically consistent, unbiased, and normally distributed. Classical least squares when psi equals x, but this is not robust to outliers. Some more robust examples follow.</p>
<p><strong>Trimmed estimator</strong> - trims outliers with large residuals</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="gsjVjGdxhbh5">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/gsjVjGdxhbh5/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/gsjVjGdxhbh5/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/gsjVjGdxhbh5/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/gsjVjGdxhbh5/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/gsjVjGdxhbh5/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/gsjVjGdxhbh5/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/gsjVjGdxhbh5/images/537.webp 537w" type="image/webp" sizes="(max-width: 537px) 100vw, 537px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/gsjVjGdxhbh5/images/537.png" width="537" height="179">
    </picture>
</figure>
<p><strong>Huber estimator</strong> - keeps all points but sets points with large residuals to a chosen value. The optimal value is 1.345 for normal distributions.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="-z9Z-aP44g7F">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/560.webp 560w,https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/561.webp 561w" type="image/webp" sizes="(max-width: 561px) 100vw, 561px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/-z9Z-aP44g7F/images/561.png" width="561" height="243">
    </picture>
</figure>
<p><strong>Tukey's bisquare/biweight function</strong> - trims very large outliers and downweights intermediate outliers. For this reason, it is a <strong>redescending</strong> M-estimator.</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="KrFJd5rGmcYw">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/KrFJd5rGmcYw/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/KrFJd5rGmcYw/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/KrFJd5rGmcYw/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/KrFJd5rGmcYw/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/KrFJd5rGmcYw/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/KrFJd5rGmcYw/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/KrFJd5rGmcYw/images/527.webp 527w" type="image/webp" sizes="(max-width: 527px) 100vw, 527px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/KrFJd5rGmcYw/images/527.png" width="527" height="124">
    </picture>
</figure>
<p><strong>Least absolute deviation estimator</strong> - minimizes</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="3tSXo_aQP95B">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/3tSXo_aQP95B/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/3tSXo_aQP95B/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/3tSXo_aQP95B/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/3tSXo_aQP95B/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/3tSXo_aQP95B/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/3tSXo_aQP95B/images/442.webp 442w" type="image/webp" sizes="(max-width: 442px) 100vw, 442px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/3tSXo_aQP95B/images/442.png" width="442" height="194">
    </picture>
</figure>
<p><strong>Least trimmed squares estimator</strong> - minimizes</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="sVUHrD0Su_W_">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/sVUHrD0Su_W_/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/sVUHrD0Su_W_/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/sVUHrD0Su_W_/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/sVUHrD0Su_W_/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/sVUHrD0Su_W_/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/sVUHrD0Su_W_/images/474.webp 474w" type="image/webp" sizes="(max-width: 474px) 100vw, 474px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/sVUHrD0Su_W_/images/474.png" width="474" height="204">
    </picture>
</figure>
<p>where q = (n + p + 1)/2, and p is the number of parameters. LTS is very robust.</p>
<p><strong>Thiel-Sen median slope</strong> - find the median of the n(n+1)/2 slopes of lines defined <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">by all pairs of data points.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">7.4 Weighted least squares</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">It is common to get uncertainty measurements on each individual data point. This changes regression.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Minimum χ<sup>2</sup> regression </strong>- a weighted least-squares regression procedure, requires minimizing</span></p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="RTvKsOa5jaVK">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/RTvKsOa5jaVK/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/RTvKsOa5jaVK/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/RTvKsOa5jaVK/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/RTvKsOa5jaVK/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/RTvKsOa5jaVK/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/RTvKsOa5jaVK/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/RTvKsOa5jaVK/images/538.webp 538w" type="image/webp" sizes="(max-width: 538px) 100vw, 538px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/RTvKsOa5jaVK/images/538.png" width="538" height="147">
    </picture>
</figure>
<p><strong>Pearson's </strong><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>χ<sup>2</sup>&nbsp;test</strong> - given the null hypothesis that the data are drawn from a population following a given model, the probability that the χ<sup>2</sup>&nbsp;statistic for n observations exceeds a chosen value is asymptotically equal to the probability that a χ<sup>2</sup>&nbsp;random variable with k-p-1 degrees of freedom exceeds x. Created originally for the multinomial experiment.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">7.5 Measurement error models</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">There is often very well-defined uncertainties on both X and Y.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Measurement error/errors-in-variables/latent variable models</strong> - Regression models that use a hierarchical structure to allow for both known and unknown measurement errors.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Latent</strong> - variables in the regression equation</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Manifest</strong> - variables X and Y in the measurement equations.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">These types of models are very flexible.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Orthogonal distance regression</strong> - algorithm to minimize the squared sum of distances for data with constant error, available for both linear and nonlinear functions.</span></p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="NXH-s4rBWHJd">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/NXH-s4rBWHJd/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/NXH-s4rBWHJd/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/NXH-s4rBWHJd/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/NXH-s4rBWHJd/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/NXH-s4rBWHJd/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/NXH-s4rBWHJd/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/NXH-s4rBWHJd/images/530.webp 530w" type="image/webp" sizes="(max-width: 530px) 100vw, 530px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/NXH-s4rBWHJd/images/530.png" width="530" height="91">
    </picture>
</figure>
<p><strong>Simulation-extrapolation (SIMEX) algorithm</strong> - Monte Carlo procedure to reduce least-squares regression biases arising from measurement errors.It simulates datasets based on the observations with a range of artificially increased measurement errors.The parameter of interest is estimated <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">for each simulation. Plot the estimated parameter versus the total measurement error. The resulting curve is extrapolated to zero measurement error.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Maximum likelihood procedures are in general more flexible than least-squares for regression, though they are more difficult to use.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">7.6 Nonlinear models</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">One common method for regression is least-squares estimation that minimizes the residual sum of squares with respect to the vector of parameters.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Poisson regression</strong> - when response variable is a result of a Poisson process. Events are assumed independent. Takes advantage of nonnegativity and integer nature.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Logistic regression</strong> - when response variable is binary. Assumes that the <strong>logit</strong> is a linear function of the independent variables:</span></p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="IxVRGFYRp9lK">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/IxVRGFYRp9lK/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/IxVRGFYRp9lK/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/IxVRGFYRp9lK/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/IxVRGFYRp9lK/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/IxVRGFYRp9lK/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/IxVRGFYRp9lK/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/IxVRGFYRp9lK/images/527.webp 527w" type="image/webp" sizes="(max-width: 527px) 100vw, 527px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/IxVRGFYRp9lK/images/527.png" width="527" height="195">
    </picture>
</figure>
<p>Regression coefficients can be obtained by MLE, maximizing</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="Kk6iTwkFElac">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/Kk6iTwkFElac/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/Kk6iTwkFElac/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/Kk6iTwkFElac/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/Kk6iTwkFElac/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/Kk6iTwkFElac/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/Kk6iTwkFElac/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/Kk6iTwkFElac/images/539.webp 539w" type="image/webp" sizes="(max-width: 539px) 100vw, 539px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/Kk6iTwkFElac/images/539.png" width="539" height="175">
    </picture>
</figure>
<h3>7.7 Model validation, selection and misspecification</h3>
<p>Getting best-fit parameters does not guarantee that the model actually fits the data. Most of the time, fits are not unique.</p>
<p><strong>Coefficient of determination</strong> - A good model should account for most of the scatter in the original data. This is quantified in the R<sup>2</sup> statistic,</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="w7dKMjCarJ-7">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/w7dKMjCarJ-7/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/w7dKMjCarJ-7/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/w7dKMjCarJ-7/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/w7dKMjCarJ-7/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/w7dKMjCarJ-7/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/w7dKMjCarJ-7/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/w7dKMjCarJ-7/images/536.webp 536w" type="image/webp" sizes="(max-width: 536px) 100vw, 536px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/w7dKMjCarJ-7/images/536.png" width="536" height="158">
    </picture>
</figure>
<p>, the ratio of the <strong>error sum of squares</strong> to the <strong>total sum of squares</strong>. A successful model has R<sup>2</sup> approach 1.&nbsp;</p>
<p><strong>Adjusted R<sup>2</sup></strong> - penalizes number of parameters&nbsp;</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="VTYPxY_8K6MZ">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/VTYPxY_8K6MZ/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/VTYPxY_8K6MZ/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/VTYPxY_8K6MZ/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/VTYPxY_8K6MZ/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/VTYPxY_8K6MZ/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/VTYPxY_8K6MZ/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/VTYPxY_8K6MZ/images/514.webp 514w" type="image/webp" sizes="(max-width: 514px) 100vw, 514px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/VTYPxY_8K6MZ/images/514.png" width="514" height="185">
    </picture>
</figure>
<p><strong>Cross-validation</strong> - a portion of the dataset is withheld and used to evaluate the regressed model. When CV is repeated more and more times, it approaches jackknifing.</p>
<h3>7.8 Remarks</h3>
<p>Most astronomers don't think very hard about the choices they make during regression, but they should.</p>
<h3>R applications</h3>
<p>This chapter has many examples. There is linear regression with heteroscedastic measurement <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">errors, linear modeling using ordinary and weighted least squares, generalized linear modeling using glm, robust regression using robust M-estimators and Thiel-Sen, and linear and non-linear quantile regression.</span></p>
<h2><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Multivariate analysis</span></h2>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">8.1 The astronomical context</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Multivariate data is very common in astronomy (e.g. data tables)</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">8.2 Concepts of multivariate analysis</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">The goals of multivariate analysis are to:</span></p>
<ol>
    <li><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Quantify the location and distribution of the dataset in p-dimensional space</span></li>
    <li><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Simplify the data by reducing the number of variables without significant information loss</span></li>
    <li><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Investigate dependence between variables</span></li>
    <li><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Do statistical inference (hypothesis testing, confidence regions, etc.)</span></li>
    <li><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Classification and clustering</span></li>
    <li><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Predict locations of future objects (though this isn't usually relevant to astronomy)</span></li>
</ol>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Covariance matrix</strong> - measures how variables vary with each other.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">There are many different choices for defining distances between datapoints in p space. When all the units are the same, the natural metric is the</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Euclidean distance</strong> - flat space distance</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Minkowski metric</strong> - generalization of Euclidean distance for different m values</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="rdnkP_Xskwo9">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/560.webp 560w,https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/640.webp 640w,https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/667.webp 667w" type="image/webp" sizes="(max-width: 667px) 100vw, 667px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/rdnkP_Xskwo9/images/667.png" width="667" height="156">
    </picture>
</figure>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="HfOPlA7rlbBo">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/560.webp 560w,https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/640.webp 640w,https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/641.webp 641w" type="image/webp" sizes="(max-width: 641px) 100vw, 641px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/HfOPlA7rlbBo/images/641.png" width="641" height="167">
    </picture>
</figure>
<p>m = 1 is the <strong>Manhattan distance</strong> - distance measured only along grid lines. m = 2 is Euclidean, and m going to infinity gives the maximum or Chebyshev distance.</p>
<p>Can use weighted distances if measurement errors are known.</p>
<p><strong>Distance matrix</strong> - matrix with all the distances between each element.</p>
<p>In astronomy, this is difficult because the variables do not have the same units. An easy solution is to standardize all variables by offsetting each to zero mean and unit standard deviation and then use euclidean distance.</p>
<p><strong>Multivariate normal distribution</strong> - the natural extension of the normal distribution</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="e5vQFQWHTlOf">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/560.webp 560w,https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/640.webp 640w,https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/668.webp 668w" type="image/webp" sizes="(max-width: 668px) 100vw, 668px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/e5vQFQWHTlOf/images/668.png" width="668" height="117">
    </picture>
</figure>
<p>mu is the population mean, sigma is the population covariance matrix (', -1, || represent transpose, inverse, and determinant, respectively).</p>
<h3>8.5 Hypothesis tests</h3>
<p><strong>Hotelling T<sup>2</sup></strong> - multivariate generalization of student's t</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="YKI-uB1heY6I">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/YKI-uB1heY6I/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/YKI-uB1heY6I/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/YKI-uB1heY6I/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/YKI-uB1heY6I/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/YKI-uB1heY6I/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/YKI-uB1heY6I/images/474.webp 474w" type="image/webp" sizes="(max-width: 474px) 100vw, 474px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/YKI-uB1heY6I/images/474.png" width="474" height="75">
    </picture>
</figure>
<p><strong>Wilks' lambda</strong> - generalization of likelihood ratio test</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;" data-ckbox-resource-id="bzuRaM634IAs">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/480.webp 480w,https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/560.webp 560w,https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/640.webp 640w,https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/643.webp 643w" type="image/webp" sizes="(max-width: 643px) 100vw, 643px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/bzuRaM634IAs/images/643.png" width="643" height="116">
    </picture>
</figure>
<h3>8.4 Relationships among the variables</h3>
<p>It is straightforward to extend linear regression to multivariate data, assuming only one response variable. Just replace X by a matrix and beta with a vector.</p>
<p>The extension of the least-squares estimator for the regression coefficients is&nbsp;</p>
<figure class="image image-style-block-align-left" data-ckbox-resource-id="N_4WwVimR6hS">
    <picture>
        <source srcset="https://ckbox.cloud/173cad3a157ac9223907/assets/N_4WwVimR6hS/images/80.webp 80w,https://ckbox.cloud/173cad3a157ac9223907/assets/N_4WwVimR6hS/images/160.webp 160w,https://ckbox.cloud/173cad3a157ac9223907/assets/N_4WwVimR6hS/images/240.webp 240w,https://ckbox.cloud/173cad3a157ac9223907/assets/N_4WwVimR6hS/images/320.webp 320w,https://ckbox.cloud/173cad3a157ac9223907/assets/N_4WwVimR6hS/images/400.webp 400w,https://ckbox.cloud/173cad3a157ac9223907/assets/N_4WwVimR6hS/images/471.webp 471w" type="image/webp" sizes="(max-width: 471px) 100vw, 471px"><img src="https://ckbox.cloud/173cad3a157ac9223907/assets/N_4WwVimR6hS/images/471.png" width="471" height="100">
    </picture>
</figure>
<p><strong>Ill-conditioning</strong> - Linear regression is unstable when variables are colinear</p>
<p><strong>Least absolute shrinkage and selection operation/lasso</strong> - least squares regression model based on robust constraints.</p>
<p><strong>Principal components analysis</strong> - models covariance structure with linear combinations of the variables. Finds linear relationships that account for the most variance in the data. It simultaneously studies linear relationships between properties and reduces dimensionality.</p>
<p>First center the data, then make covariance matrix, find evals and evecs of covariance matrix (these are variances and PCA components, respectively), plot each data point in PCA space. Classify data by looking for clusters or make images of each PCA component, then finally determine the physical meaning of the PCA vectors.</p>
<p><strong>Independent component analysis</strong> - Assume that the underlying sources are independent. want to maximize negentropy. Use PCA to reduce dimensionality, then find the independent components. They are uncorrelated, but not perpendicular. However, they are independent.</p>
<h3>8.5 Multivariate visualization</h3>
<p>Bivariate histograms,<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"></span> scatter plots, and contour plots are common in astronomy. One can also use a matrix of univariate or bivariate plots</p>
<p>R has some different ways of visualizing higher-dim data, but we never really went over this section in class.</p>
<h3>R applications</h3>
<p>There are examples in the book of univariate tests of normality and boxplots and Q-Q plots. They give a tutorial on how to prepare a multivariate dataset for analysis. They demonstrate bivariate relationships with a pair plot. There is an example of PCA as well, along with multiple regression and MARS nonlinear regression. There are also some techniques of multivariate visualization techniques on display.</p>
<h2>Clustering, classification, and data mining</h2>
<p>Subpopulations show up all the time in multivariate data</p>
<h3>9.1 The astronomical context</h3>
<p>Astronomers are always classifying things into different groups and classes based on various properties. However, most of the old classifications were subjective. There are also objective and statistical ways of doing this, even if the human eye is very good at it naturally.</p>
<h3>9.2 Concepts of clustering and classification</h3>
<p>Assume all datasets consist of n objects with p properties, with members from k subpopulations.</p>
<p><strong>Unsupervised clustering techniques</strong> - Used when the number, location, size, and/or mophologies of the groupings are unknown. They look for groupings based on proximity using some distance measure in p-space. Results are uncertain and depend on distance measure.</p>
<p><strong>Supervised classification</strong> - the combination of using a training set to characterize the groupings in the dataset and establishing discrimination rules on the training set and applying them to a new test set.</p>
<p><strong>classification/decision trees</strong> - fall into subgroups based on different ranges of multiple variables. a better definition later.</p>
<p><strong>Normal mixture model</strong> - the dataset is assumed to consist of k multivariate Gaussians. MLE or Bayesian are used to obtain the number of groups and their mean and variance.</p>
<p><strong>Data mining</strong> - methods of classifying objects in very large datasets</p>
<p><strong>Machine learning</strong> - allow the system to improve its own performance as more data are treated. There are many methods, including both unsupervised and supervised classification using decision trees, neural networks, support vector machines, and Bayesian networks.</p>
<p><strong>Center</strong> - necessary for many clustering and classification algorithms. The natural choice is the centroid, but the median is more robust, so use <strong>medoids</strong>.</p>
<p>It is important to be able to measure the success of any given procedure. The probability of misclassification can be combined with asymmetries as the <strong>cost</strong> of misclassification. A big question in science is how to balance the cost. For example, if the sample is very large, it would be more costly to waste time on erroneous measurements, but if the sample is small, it might be more costly to lose possible targets.</p>
<p><strong>Expected cost of misclassification</strong> - exactly what it sounds like</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:640/75;" src="https://i.ibb.co/Dk8rdMP/image.png" width="640" height="75"></figure>
<h3>9.3 Clustering</h3>
<p><strong>Hierarchical clustering methods</strong> - find groupings without parametric assumptions or prior knowledge. It beings with each point in its own cluster. The clusters that are closest together are merged. This is repeated n-1 times until the entire dataset is within a single cluster.</p>
<p><strong>Tree/dendrogram</strong> - The result of hierarchical clustering. It shows every step of the process and describes how and when each cluster merged.</p>
<p>HC is very sensitive to the distance metric chosen. There are a few common choices:</p>
<p><strong>Single linkage/friends-of-friends/nearest-neighbor clustering</strong> - the distance metric is the distance to the closest cluster member. Tends to produce unphysical elongated clusters.</p>
<p><strong>Minimal spanning tree</strong> - subset of edges on a graph that connect all points with the minimum total weight (usu. distance)</p>
<p><strong>Complete linkage</strong> - distance to the furthest cluster member is the metric</p>
<p><strong>Average linkage</strong> - intermediate choice where the distance metric is the average distance to cluster members</p>
<p><strong>Ward's minimum variance</strong> - Clusters are merged to minimize the increase the sum of intra-cluster summed squared distances.</p>
<p><strong>k-means partitioning</strong> - start with k <strong>seed</strong> locations that represent the cluster centroids and iteratively reassign each object to the nearest cluster to reduce the sum of within-cluster squared distances. Usually converges rapidly. Choices that must be made: number of clusters k, initial seed locations (can have some impact), and must define the centroid. Choosing mean is sensitive to outliers, so sometimes use <strong>k-medoid partitioning</strong>. Careful that the summed distances, not squared distances, are minimized.</p>
<h3>9.4 Clusters with substructure or noise</h3>
<p>Previous methods assume all objects belong to a cluster, but, in astronomy especially, this is not always the case, such as background stars.</p>
<p><strong>Bump hunting</strong> - start with the full dataset, progressively shrink a box with sides parallel to the axes. The box face is contracted along the face that most increases the mean density or statistic. After the peak location is located, the box may be expanded if density increases. Repeat to find other bumps.</p>
<p><strong>DBSCAN (the best)</strong> - requires a minimum number of points within a specified <strong>reach</strong> distance. A cluster around an object with at least the minimum number of points within the reach expands naturally to include all density-reachable points. This continues until the criteria are broken. Unclassified points are considered noise.</p>
<p>There are other similar methods such as <strong>OPTICS, BIRCH, DENCLUE, CHAMELEON, etc.</strong></p>
<h3>9.5 Mixture models</h3>
<p>Distinction between white noise and peaked distributions is bump hunting and distinction between one or more peaks is a test for <strong>multimodality</strong>.</p>
<p><strong>Mixture models</strong> - Data are assumed to consist of some number of a given model form. Normal mixture models assume data are in k ellipsoidal clusters with MVN distributions.</p>
<p>There is a probabilistic method to assigning objects to more than one cluster (<strong>soft v. hard</strong>).</p>
<h3>9.6 Supervised classification</h3>
<p>The most common model for supervised classification is MVN distribution.</p>
<p><strong>Linear discriminant analysis</strong> - LDA for two classes is geometrically the same as projecting p-dim data point cloud onto a 1D line in p-space that maximally separates the classes. Separation is measured by the ratio of between-cluster variance B to the within-cluster variance W. Maximum separation occurs for</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:640/254;" src="https://i.ibb.co/qpHLHvR/image.png" width="640" height="254"></figure>
<p>LDA is similar to PCA but has a different purpose. PCA finds linear combinations of variables that explain the most variance for the whole sample, and LDA finds linear combinations that separate classes within the sample. LDA vector a is obtained from a training set before being applied to new data.</p>
<p><strong>Support Vector Machines</strong> - class of generalized LDAs. Dataset mapped by nonlinear functions onto a higher dimensional space so that classes in the training set can be separated using linear discrimination.</p>
<p><strong>Classification trees</strong> - dividing a multivariate training sample into classes by progressive splitting</p>
<p><strong>k-nearest-neighbor/memory-based</strong> <strong>classifier</strong> - related to nearest-neighbor and local regression density estimation techniques. Cluster membership is determined by the memberships of the k nearest-neighboring points in the training set. <strong>DANN</strong> is analogous to DBSCAN.</p>
<p><strong>Automated neural networks </strong>- provide an environment where LDs are obtained in a space that has been nonlinearly mapped to from the original variables. weights learned from the training set. Basically, they use nonlinear rules to find classes.</p>
<h3>R applications</h3>
<p>There are many good examples in this chapter. Including unsupervised clustering using hierarchical clustering and dendrograms, DBSCAN, mixture models, and model based clustering. There is also supervised classification using LDA, k-nn, and single layer neural networks. There are examples of CART and SVM.</p>
<h2>Nondetections: censored and truncated data</h2>
<h3>10.1 The astronomical context</h3>
<p>Astronomy is an unusual scientific discipline because there are a lot of upper limits on data points, and truncation by sensitivity limits is very common.</p>
<h3>10.2 Concepts of survival analysis</h3>
<p><strong>Censoring</strong> - object is known to exist but the object is undetected in the desired property. Can be <strong>left-censored</strong> (upper limits) or <strong>right-censored</strong> (lower limits)</p>
<p><strong>Random left-censoring</strong> - some extraneous factor causes objects to be undetected (like poor sky conditions on a telescope)</p>
<p><strong>Truncation</strong> - undetected objects are completely missing from the dataset.&nbsp;</p>
<p><strong>Survival function</strong> - the probability that an object has a value above some specified level</p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:522/201;" src="https://i.ibb.co/SKvqNNK/image.png" width="522" height="201"></figure>
<p><strong>Hazard rate</strong> - the derivative of the survival function. Also, the pdf divided by the survival function.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:640/198;" src="https://i.ibb.co/YjR6Rsv/image.png" width="640" height="198"></figure>
<p><strong>Proportional hazard model</strong> - A hazard function with an exponential decay h(x) = h<sub>0</sub>e<sup>-βx</sup>.</p>
<p><strong>Cox regression</strong> - estimation of the β dependencies</p>
<h3>10.3 Univariate datasets with censoring</h3>
<p>If the distribution is known already, ML methods can be used, even with censoring and truncation. The likelihood is given by the following</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:640/108;" src="https://i.ibb.co/mSTBqgT/image.png" width="640" height="108"></figure>
<p>Once L is established, all of the tools from chapter 3 are applicable. There are some special cases given in the book where the solutions are particularly easy.</p>
<p>Nonparametric estimation of censored CDFs are straightforward generalizations <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">of the edf.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">When left-censored, the cumulative hazard function is&nbsp;</span></p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:505/193;" src="https://i.ibb.co/N3RBTLG/image.png" width="505" height="193"></figure>
<p>where d is the number of objects at value xi (1 without ties).</p>
<p><strong>Kaplan-Meier estimator</strong> - Does not change values at censored data. with left-censored data, the size of the jumps grows at lower values as the number of remaining objects becomes small</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:596/188;" src="https://i.ibb.co/mJXgFN6/image.png" width="596" height="188"></figure>
<p>Can obtain the KM estimator by a <strong>redistribute</strong> algorithm, where the weight of each upper limit is redistributed equally among the lower detections.</p>
<p>Can use two sample tests to ask “what is the probability that two censored datasets do not come from the same distribution.” Generally the same as classical nonparametric two-sample tests but with weights (<strong>scores</strong>) assigned to censored data.</p>
<p><strong>Gehan test</strong> - generalized Wilcoxon test</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:562/205;" src="https://i.ibb.co/SwYFf5K/image.png" width="562" height="205"></figure>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:633/131;" src="https://i.ibb.co/TMNVNGh/image.png" width="633" height="131"></figure>
<p>General statistic for testing the null hypothesis that the k hazard rates are the same</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:605/111;" src="https://i.ibb.co/KWphbXv/image.png" width="605" height="111"></figure>
<p>where y is the number of objects at risk (not yet detected), d is the number of ties in the jth sample at value xi. N is total number of objects, j denotes individual samples. There are many common choices for the weights W.</p>
<p><strong>Logrank test</strong> - W = yi, the number of objects not yet detected. Equals the sum of the failures observed minus the conditional failures expected computed at each failure time (the difference between observed and expected failures in one of the groups).</p>
<p><strong>Gehan's test</strong> - W = yi<sup>2</sup></p>
<p><strong>Peto-Peto test</strong> - W = yi times the KM estimator.</p>
<p><strong>Fleming-Harrington tests</strong> - use various functions of the KM estimator as weights.</p>
<p><i>logrank is more powerful than generalized Wilcoxon when proportional hazard applies, while generalized Wilcoxon are more effective than logrank when sample differences are greater at the uncensored side of the distribution</i>.</p>
<h3>10.4 Multivariate datasets with censoring</h3>
<p>First use a hypothesis test for correlation like Spearman's rho or Kendall's tau. With censoring, the generalizations of Kendall's tau are preferred</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:640/172;" src="https://i.ibb.co/PxGHqzj/image.png" width="640" height="172"></figure>
<p>nc is the number of <strong>concordant </strong>pairs (with positive slope in x,y). nc is number of <strong>disconcordant</strong> pairs (with negative slopes). nt is the number of ties.</p>
<p>There are many methods for linear regression with censoring.</p>
<p><strong>Accelerated failure-time model</strong> - and example of <strong>exponential regression</strong>, a standard linear regression model after a logarithmic transformation of the dependent variable. Helps with censored variable has a wide range of values (over orders of magnitude)</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:494/94;" src="https://i.ibb.co/4J8Kgvg/image.png" width="494" height="94"></figure>
<p><strong>Proportional hazards model/Cox regression</strong> - mentioned before, has exponential hazard rate. For two objects with different covariate values, the hazard rates are proportional to each other as the ratio of the exponential parts.</p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:449/83;" src="https://i.postimg.cc/sfTZcPsG/image.png" width="449" height="83"></figure>
<p><strong>Iterative least squares</strong> - familiar additive linear regression model is appropriate if the censored variable and covariates have similar ranges.</p>
<p><strong>Buckley-James line estimator</strong> - variant to the MLE linear regression. permits non-gaussian residuals around the line. Trial regression is done, nonparametric KM estimator is computed and used as the distribution for the scatter. censored data points are moved downwards <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">to new locations based on KM. likelihood maximized to obtain new estimates, and the procedure is iterated.</span></p>
<p><strong>Akritas-Thiel-Sen</strong> - works well with doubly censored data or with bad outliers</p>
<h3>10.5 Truncation</h3>
<p>No knowledge of how many undetected sources exist (flux limited surveys).</p>
<p>If the underlying distribution is known, then parametric analysis can proceed based on likelihood</p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:452/206;" src="https://i.postimg.cc/Bn6tRzf1/image.pnghttps://i.postimg.cc/Bn6tRzf1/image.png" width="452" height="206"></figure>
<p>where u are the tructation limits, f is the pdf, and S is the survival.</p>
<p><strong>Lynden-Bell-Woodroofe estimator</strong> - nonparametric MLE for randomly truncated data. Unbiased, consistent, generalized MLE of underlying distribution</p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:405/124;" src="https://i.postimg.cc/CKSrFbJD/image.png" width="405" height="124"></figure>
<h3>R applications</h3>
<p>Here there are examples of the <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">KM estimator, applying two-sample tests with censoring, multidimensional problems with censoring, and the LBW estimator for truncation.</span></p>
<h2><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Time series analysis</span></h2>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">11.1 The astronomical context</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Time-domain astronomy focuses on studying variable phenomena. However, much difficulty comes from the fact that a lot of astronomy nata is not evenly spaced in time and measurements often have heteroscedastic errors.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">11.2 Concepts of time series analysis</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Procedures for evenly spaced data are well-known</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Trend</strong> - when the average value of a time series changes with time. A global linear trend has the same form as linear regression, though X is the response and t is the independent variable, for times series analysis</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Low-pass filter</strong> - smoothing to reduce variance from short-term variations</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>high-pass filter</strong> - examining residuals after fitting a regression model to reduce long-term trends</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Differencing filter</strong> - commonly used to remove various types of trends (global, periodic, stochastic, etc.) to reveal short-term structure. One example is plotting the difference between consecutive points instead of the points themselves.</span></p>
<p><strong>State-space model</strong> - &nbsp;mathematical model with input, output and state variables related by first-order ODEs or difference equations. can be constructed to fit simultaneously long-term ternds, periodicities, and short-term autocorrelation.</p>
<p><strong>Autocorrelation</strong> - measure of correlated structure in a time series. Basically, how well a signal correlates with a delayed version of itself.</p>
<p><strong>Autocorrelation function</strong> - quantifies how values at different separations (in time) vary together</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:733/161;" src="https://i.postimg.cc/cJJFHvbm/image.png" width="733" height="161"></figure>
<p>where k is the <strong>lag time</strong> and is an integer, the numerator is the sample autocovariance function, and the denominator is the sample variance. Strong ACF values for a given k imply autocorrelation on that time scale.</p>
<p><strong>Correlogram</strong> - the plot of ACF(k) for a range of k values. Often have confidence intervals for the null hypothesis that no correlation is present.</p>
<p><strong>White noise</strong> - random uncorrelated noise (usu. gaussian). It produces near-zero values in the ACF (ACF(0) = 1, trivially).</p>
<p><strong>Long-term memory</strong> - What it sounds like. Time series with this property have non-zero ACF values for a wide range of k.</p>
<p>Periodic variations in the time series produce periodic variations in the ACF - natural application to use Fourier transform.</p>
<p><strong>Residual time series</strong> - TS after trends have been removed. This is done because ACFs are difficult to interpret when trends and stochastic variations are both present.</p>
<p><strong>Stationarity</strong> - temporal behavior that is unchanged by an arbitrary time shift.</p>
<p>The concept of an average is not relevant to nonstationary processes. A concept that is often applied to residuals after a model has been fitted.</p>
<p><strong>Weakly stationary</strong> - functions with constant moments, like the mean and autocovariance but that change in other ways.</p>
<p><strong>Nonstationarity</strong> - Opposite of stationarity, clearest examples are systems that change abruptly.</p>
<p><strong>Change-point</strong> - The point in time when the variability characteristics abruptly change.</p>
<p><strong>Spectral/Fourier analysis</strong> - the transformation of a signal from t<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">he time domain to the frequency domain. Signals from periodic phenomena are much more concentrated in frequency space.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">11.3 Time-domain analysis of evenly spaced data</span></h3>
<p>Smoothing methods (Chapter 6) can very simply be applied to (evenly-spaced) time series data. One of the simplest methods is</p>
<p><strong>Central moving average</strong> - calculates the moving average within a certain window, across the entire dataset.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:676/195;" src="https://i.postimg.cc/QCv2gpRh/image.png" width="676" height="195"></figure>
<p>where the bandwidth is j time intervals. This can be modified to use median instead of mean, using only past values, weighting each value, etc.</p>
<p><strong>Exponentially weighted moving average</strong> - used for time series with short-term autocorrelation. It is called as such because <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">the current value is&nbsp;a weighted average of all previous values with weights decreasing exponentially with time.</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:729/93;" src="https://i.postimg.cc/PqK0VGB6/image.png" width="729" height="93"></figure>
<p>Variance is increased when autocorrelation is present. This can be thought of as a decrease in the number of independent measurements because the process is periodic.</p>
<p><strong>Partial autocorrelation function</strong> - Gives autocorrelation while removing the effects of correlations at shorter lags. The value of PACF(p) is found by successively fitting autoregressive models with order 1,2,…,p and setting the last coefficient of each model to the PACF parameter. For example, AR(2) has&nbsp;</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:694/137;" src="https://i.postimg.cc/tCywb4pr/image.png" width="694" height="137"></figure>
<p><strong>Lag k scatter plot</strong> - plot all x(t) against x(t+k). Random scatter implies uncorrelated noise and linear relationship points toward stochastic AR behavior. Circular distribution suggests a periodic signal, and clusters of points suggest nonstationary time series with classes of variable behavior.</p>
<p><strong>Durbin-Watson statistic</strong> - a simple measure of autocorrelation in evenly spaced time series. Value of 2 means no AC, higher is positive, lower is negative.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:674/182;" src="https://i.postimg.cc/G3STwHFR/image.png" width="674" height="182"></figure>
<p><strong>Random walk</strong> - the simplest stochastic autocorrelated model, where each step is the previous plus a guassian white noise term. Starts with an ACF of 1 that decreases with increasing k.</p>
<p><strong>Autoregressive model</strong> - Generalization of the random walk that permits dependencies on more than just the previous step (with different weights). Has ACF(k) = the first constant to the kth power.</p>
<p><strong>Moving average model</strong> - current value depends on past values of the <i>noise</i> rather than the variable itself.</p>
<p><strong>ARMA(p,q) model</strong> - A combination of AR and MA models.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:673/41;" src="https://i.postimg.cc/HkSk5N6g/image.png" width="673" height="41"></figure>
<p><strong>Autoregressive integrated moving average model</strong> - combines ARMA and random walk, since ARMA is stationary.</p>
<p><strong>Volatility</strong> - another word for heteroscedasticity.</p>
<p><strong>(Generalized) Autoregressive conditional heteroscedastic model</strong> - variance is assumed to be a stochastic autoregressive process depending on previous values.</p>
<h3>11.4 Time-domain analysis of unevenly spaced data</h3>
<p>Occurs frequently in astronomy.</p>
<p><strong>Unbinned discrete correlation function</strong> - computes autocorrelation function in a way that avoids interpolating unevenly spaced data on a regular grid.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:605/197;" src="https://i.postimg.cc/D0FTRQx6/image.png" width="605" height="197"></figure>
<p>where x and z are datasets and t is the time.</p>
<p><strong>Discrete correlation function</strong> - average of the UDCF</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:696/218;" src="https://i.postimg.cc/L4BRL7sb/image.png" width="696" height="218"></figure>
<p><strong>Structure function</strong> - measure of autocorrelation. average over the time series of the difference between x<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">(t) and x(t+tau) </span>to the qth power.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:695/93;" src="https://i.postimg.cc/R0HxFp8G/image.png" width="695" height="93"></figure>
<p>when q = 2, this is known as the <strong>variogram</strong> - alternative to ACF (more on this next chapter)</p>
<h3>11.5 Spectral analysis of evenly spaced data</h3>
<p>In Fourier analysis, a time series that is dominated by sinusoidal patterns can readily be modeled as</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:632/156;" src="https://i.postimg.cc/Y0Xs8QZh/image.png" width="632" height="156"></figure>
<p><strong>Power spectrum</strong> - the fourier transform of the autocovariance function</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:662/257;" src="https://i.postimg.cc/Zqrs2GXk/image.png" width="662" height="257"></figure>
<p><strong>Spectral analysis</strong> - power spectrum usually must be estimated from the time series measurements of evenly spaced data. Usually with the finite fourier series</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:714/86;" src="https://i.postimg.cc/kX0cCjQ7/image.png" width="714" height="86"></figure>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:581/422;" src="https://i.postimg.cc/NjRxj1Dm/image.png" width="581" height="422"></figure>
<p><strong>Periodogram</strong> - a histogram of the quantity I(omega) against omega</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:695/140;" src="https://i.postimg.cc/SKr7WkwK/image.png" width="695" height="140"></figure>
<p>The periodogram has limitations: high noise, biased, unreasonable assumptions (infinitely long and evenly spaced data with stationary sinusoidal signals)</p>
<p><strong>Spectral leakage</strong> - power going into nearby frequencies when the time series is short compared to the period.</p>
<p>Can smooth to reduce variance in either the frequency or time domains.</p>
<h3>11.6 Spectral analysis of unevenly spaced data</h3>
<p>Occurs more in astronomy than many other fields.</p>
<p><strong>Lomb-Scargle periodogram</strong> - generalization of the periodogram for unevenly spaced data. Can be either a modified Fourier analysis or a least-squares regression to sine waves</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:711/85;" src="https://i.postimg.cc/hvVyKHnJ/image.png" width="711" height="85"></figure>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:334/76;" src="https://i.postimg.cc/GmL71752/image.png" width="334" height="76"></figure>
<p><strong>Minimum string length</strong> - The sum of the length of lines connecting values of the time series as the phase runs from 0 to 2pi. For an incorrect period, the signal is scattered in phase and the string length is large.</p>
<p><strong>Analysis of variance (ANOVA) statistic</strong> - ratio of the sum of inter-bin variances to the sum of intra-bin variances</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:682/136;" src="https://i.postimg.cc/YSbwHLRc/image.png" width="682" height="136"></figure>
<p>Significance levels are in general difficult to determine for periodigrams of unevenly spaced data, but for the LSP, the probability that some spectral peak is higher than z0 is:</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:641/78;" src="https://i.postimg.cc/m2YLnDv4/image.png" width="641" height="78"></figure>
<p>where Np is the number of independent periods. Widely used, but it has its problems. Assumes that there is a single periodic signal in white noise and that the variance is known in advance.</p>
<p><strong>Nonstationary Poisson processes</strong> - Poisson process with underlying periodic variability. There are some periodograms that can be used in this case.</p>
<h3>11.7 State-space modeling and the Kalman filter</h3>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:683/194;" src="https://i.postimg.cc/TwdgKFnL/image.png" width="683" height="194"></figure>
<p>X is a vector of observed values for a collection of variables. <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">S is the <strong>state vector</strong> that depends on the parameters theta, h is the matrix that maps the true state space into the observed space. G is the <strong>transition matrix</strong> that defines how the system changes in time. Epsilon and η terms terms are normal errors.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">11.8 Nonstationary time series</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">There are many ways to analyze nonstationary time series:</span></p>
<p><strong>Regression for deterministic behavior</strong> - best-fit parameters can be estimated by least-squares, MLE, or Bayesian using standard regression or state space approaches.</p>
<p><strong>Bayesian blocks and bump hunting</strong> - semi-parametric regression to characterize signals and detect change-points. Data is a sequence of events. Construct a likelihood assuming a sequence of constant flux levels with jumps at a number of change-points.</p>
<p><strong>Change-point analysis</strong> - If likelihood methods establish the model of the initial state, the likelihood ratio test can find changes in the mean, variance, trend, autocorrelation, and other model characteristics. Nonparametric tests are often used to find structural changes.</p>
<p><strong>Wavelet analysis</strong> - Wavelets transform time data to frequency space like fourier, but it is well-suited to nonstationary processes, especially if variations occur on a range of temporal scales. The transform is given by</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:739/149;" src="https://i.postimg.cc/RFPky3mP/image.png" width="739" height="149"></figure>
<p>where Xt is the function being transformed, s is the scale, u is the time, and g is a simple basis function that falls rapidly to 0, like a gaussian. This process maximizes the temporal resolution at all frequencies by changing the area of each bin in frequency space (higher frequencies get better time resolution and worse frequency resolution and vice versa).</p>
<p><strong>Singular spectrum analysis</strong> - PCA for temporal data, used for smoothing complex time series.</p>
<h3>11.9 1/f noise or long-memory processes</h3>
<p><strong>1/f/flicker/red noise or long-memory process</strong> - when variance increases with the characteristic time-scale and the power spectrum can be fit by a power-law at low frequencies. It is called 1/f because there is an inverse correlation of power and frequency in the Fourier power spectrum.</p>
<p><strong>Short-memory</strong> - processes that can be understood with low-order ARMA models.</p>
<p>Long-memory processes can arise from a combination of many short-memory processes like the sound of a babbling brook. or from starspots, etc. Often modeled as an autocorrelation function that decays as a power law at large time lags.</p>
<h3>11.10 Multivariate time series</h3>
<p>Multiple variables might depend on each other in time.</p>
<p><strong>Cross-correlation function</strong> - the second-order moments for a bivariate stochastic process are the two individual ACFs and the CCF:</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:672/127;" src="https://i.postimg.cc/C1qqR8Kt/image.png" width="672" height="127"></figure>
<p>If the CCF shows a single strong peak at some lag k, this may reflect similar structure in the two time series with <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">a delay. CCF can be generalized to multivariate using correlation matrix.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">This approach is really uncommon in astronomy, so it is not discussed further.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">R applications</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Here there are examples of histograms, raw and smoothed time series, raw and smoothed periodograms, autocorrelation functions, autoregressive modeling,</span><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"></span> depictions of long-range memory, and wavelet analysis.</p>
<h2>Spatial point processes</h2>
<h3>12.1 The astronomical context</h3>
<p>Spatial data is used all the time in photometry, though the third dimension is usually much less well defined. In addition, spatial point processes are applicable to much astronomical data in low dimensions.</p>
<h3>12.2 Concepts of spatial point processes</h3>
<p><strong>Point processes</strong> - sets of irregular patterns in (usually) 2 or 3D space. <strong>Stationary </strong>if invariant under spatial translation. <strong>Isotropic </strong>if invariant under rotation</p>
<p><strong>Intensity</strong> - λ, and</p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:354/91;" src="https://i.postimg.cc/MHGXyjZq/image.png" width="354" height="91"></figure>
<p>is the expected number of points in a unit of volume v. Follows a Poisson distribution for Poisson processes</p>
<p><strong>Complete spatial randomness</strong> - a stationary Poisson point process where λ is constant across the space. Sprinkling sand grains on a table or background radiation in a detector.</p>
<p><strong>Inhomogeneous</strong> - when λ varies with location.</p>
<p><strong>Marked point processes</strong> - PP when there are associated nonspatial variables.</p>
<h3>12.3 Tests of uniformity</h3>
<p>Spatial point process analysis usually begins w<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">ith testing the hypothesis that the distribution is consistent with CSR. One thing that is often compared with CSR is the edf of the</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Nearest-neighbor distribution</strong> -&nbsp;</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:569/80;" src="https://i.postimg.cc/0QjbSwyx/image.png" width="569" height="80"></figure>
<p>where A is the 2D area containing n points. Simulations are needed to account for edge effects and for small-n samples.</p>
<p>Some CSR tests are based on sums of nearest-neighbor distances</p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:426/203;" src="https://i.postimg.cc/0jdjRdtY/image.png" width="426" height="203"></figure>
<p>where lambda is average surface density of points and W is the Euclidean distance between i-th point and its nearest neighbor.</p>
<p>A modified Pollard's P performs well using the five nearest neighbors</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:644/124;" src="https://i.postimg.cc/Pr5T5jNs/image.png" width="644" height="124"></figure>
<p>&nbsp;where j = 1 to 5 and Yij are distances between i randomly placed locations and the jth nearest neighbor.</p>
<p>Bias from edge effects is a big issue.</p>
<h3>12.4 Spatial autocorrelation</h3>
<p>The unbiased MLE for the intensity of a stationary Poisson point process is the intuitive value: the number of points in a unit p-dim volume.</p>
<p><strong>Moran's I and Geary's contiguity ratio c</strong> - Indices of spatial autocorrelation that are extensions of Pearson's coefficients of bivariate correlation. They detect departures from spatial randomness. Usually inverse distance weighted. For data grouped into m identical spatial bins</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:718/220;" src="https://i.postimg.cc/sgBB1S1r/image.png" width="718" height="220"></figure>
<p>where mj is the count of objects in the jth bin. w is the matrix of the kernel function and W is the sum of w elements.</p>
<p><strong>Correlograms</strong> - plots of I against d.</p>
<p>E[I] = -1/(n-1), E(c) = 1 for normal noise without clusters or autocorrelation. With strong autocorrelation, both approach 0</p>
<p><strong>Variogram/semi-variogram</strong> - used to map the variance on different scales across the survey area. half the average squared difference between pairs of values is plotted against the separation distance.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:633/146;" src="https://i.postimg.cc/Jnym5cVr/image.png" width="633" height="146"></figure>
<p>where m(d) is represents all of the pairs of points which are a distance d away from each other.</p>
<p>The variogram describes the se<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">cond-moment properties of a spatial process S</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:652/146;" src="https://i.postimg.cc/SRydpVSq/image.png" width="652" height="146"></figure>
<p>For a stationary, isotropic, Gaussian spatial process, the variogram simplifies to</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:585/190;" src="https://i.postimg.cc/Jhp4qzsq/image.png" width="585" height="190"></figure>
<p>where rho is the <strong>spatial covariance function</strong>, d is the distance between data points at xi and xj.</p>
<p><strong>Nugget effect</strong> - the σ<sup>2</sup> offset at zero distance in a variogram. It is because of variations smaller than the minimum separation <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">of measured points.</span></p>
<p><strong>Sill</strong> - the rise to a constant value.</p>
<p><strong>Range</strong> - the distance to the sill.</p>
<p>Variograms are usually modeled by simple functions (circular, parabolic, exponential, gaussian, power-law models exist)</p>
<p>There are <strong>local indicators of spatial association</strong> such as <strong>local Moran's I</strong> - averages the spatial variance among the k-nearest neighbors of the ith point.&nbsp;</p>
<h3>12.5 Spatial interpolation</h3>
<p>If a SPP is a representation of an underlying continuous distribution, the interpolation is an effective way of estimating the distribution. Could either interpolate just density of points, but usually you interpolate some mark variable.</p>
<p>Because many things are influenced by the objects closest to them, it makes sense to use</p>
<p><strong>Inverse distance weighted interpolation</strong> - exactly what it sounds like, the weighting scales inversely as a power law of the distance. The value z is calculated as</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:657/222;" src="https://i.postimg.cc/yYz4D3jZ/image.png" width="657" height="222"></figure>
<p>where d is the vectorial distance from the ith measurement to x0. α is chosen between 0 and 3, with smaller values giving smoother interpolations.</p>
<p>The most common method for interpolation of marked spatial data is</p>
<p><strong>Kriging</strong> - a suite of related linear least-squares, minimum variance methods for density estimation in 2 or 3 spatial dimensions that localize inhomogeneous features instead of globally characterizing homogeneous d<span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">istributions.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Ordinary kriging</strong> - want weightings at each point in the dataset to give the unbiased estimator of the continuous response variable z from measurements zi as</span></p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:496/210;" src="https://i.postimg.cc/9M6MRXgQ/image.png" width="496" height="210"></figure>
<p>There are many types of kriging available: <strong>Simple kriging</strong> if the mean is already known, <strong>block kriging</strong> when the dataset is large and adjacent points can be merged, <strong>cokriging</strong> if two correlated variables are measured at each location, <strong>universal kriging</strong> first removes trends in the data, (<strong>external drift</strong> when a secondary variable can map those trends), <strong>nonlinear kriging</strong> when the trends are not linear in the variables<strong>, multitype kriging</strong> with categorical covariates, <strong>multivariate kriging</strong> with multiple interacting marked variables.</p>
<h3>12.6 Global functions of clustering</h3>
<p>There are two very important global measures of clustering</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:633/298;" src="https://i.postimg.cc/TwJdwjbC/image.png" width="633" height="298"></figure>
<p>where these two functions are the cumulative distributions of the nearest-neighbor distances either from the data points (G) or from random locations (F)</p>
<p><strong>Empty space function</strong> - F, above, because it is centered around empty locations.</p>
<p>F works better for weak clustering and G works better for strong clustering.</p>
<p><strong>Ripley's K function</strong> - most widely used global measure of clustering. It is the average number of points within distance d of observed pointsdivided by the intensity.</p>
<p><strong>Lieshout-Baddeley J function</strong> - (1-G)/(1-F), compares inter-event distances to distances from a fixed point.</p>
<p><strong>Two-point correlation function</strong> - counts the number of objects in annuli (instead of circles) around each point. Measures the covariance of the nonuniform structure of the point distribution.</p>
<h3>12.7 Model-based spatial analysis</h3>
<p>The parametric models for spatial analysis are similar to those for regression and time series. The hypothesis of CSR can be written as <strong>X = μ + epsilon</strong>, where epsilon is Poisson noise.This has no autocorrelation, clustering, or trend.</p>
<p><strong>Geographically weighted regression</strong> - maps of several variables are available and you want to explain one map in terms of the others with a linear regression that takes spatial autocorrelation into account.</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:695/104;" src="https://i.postimg.cc/J4Q0TLdG/image.png" width="695" height="104"></figure>
<p>The response variable Y at location x depends on both the value of the <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">covariates X and on neighboring values of Y, where epsilon is normal noise. The least-squares solution is found by minimizing</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:614/110;" src="https://i.postimg.cc/6pyKdxss/image.png" width="614" height="110"></figure>
<p><strong>Spatial autoregressive model</strong> - assumes the regression coefficients are global, but the response variable is dependent on neighboring values. Can be written in a hierarchical fashion</p>
<figure class="image image-style-block-align-left"><img style="aspect-ratio:513/143;" src="https://i.postimg.cc/nLJdnqGd/image.png" width="513" height="143"></figure>
<p>where mu is the mean, delta are the <span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">spatially correlated residuals, beta between -1 and 1 gives the strength of the spatial autocorrelation, epsilon is white noise, W is a normalized weight matrix that depends on the distance d from the data point x.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">12.8 Graphical networks and tessellations</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">The p-dim distribution of neighbors provides more detailed information about local autocorrelation and clustering for point processes. Visualizations of these connections between nearby points are important to <strong>graph theory</strong>.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Delaunay triangulation</strong> - join all neighboring triplets of points with triangles.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;"><strong>Voronoi/Dirichlet/Thiessen tessellations</strong> - based on line bisectors perpendicular to the Delaunay line segments. Splits the space into polygons around the datapoint. Within a polygon, all points are closer to the associated data point (or set of data points, for higher orders) than any other (set of) point(s).</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Pruning MSTs give a procedure to define a hierarchy of spatial clusters. discussed in chapter 9.</span></p>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">12.9 Points on a circle or sphere</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Useful in astronomy, since 2D sky positions are on a sphere.</span></p>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">Moments are not generally useful because of angles. Angular values must be summed vectorially. The average direction of a sample of angles is the solution of</span></p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:684/392;" src="https://i.postimg.cc/W4z7dLd4/image.png" width="684" height="392"></figure>
<p>R/n is the <strong>mean resultant length</strong>, and it approaches 1 when the data are clustered towards one direction, and 0 when they are balanced.</p>
<p><strong>Sample circular variance</strong> - 1 - Rbar.</p>
<p><strong>Circular dispersion</strong> - a more useful measure of angular spread</p>
<figure class="image image_resized image-style-block-align-left" style="width:50%;"><img style="aspect-ratio:562/142;" src="https://i.postimg.cc/CL65w3p7/image.png" width="562" height="142"></figure>
<h3><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">R applications</span></h3>
<p><span style="box-sizing:border-box;margin:0;padding:0;text-align:left;">In this section there are 3D plots, characterization of autocorrelation with Moran's I and Geary's c, variogram analysis, characterization of clustering with K function, the two-point correlation function, examples of tessellations and applications to smoothing, spatial interpolation with IDW and ordinary kriging, discussion of spatial regression and modeling, and some examples of circular and spherical statistics.</span></p>
</body>
</html>